"""
extract_distillation_data.py - GUAVAè’¸ç•™ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã‚¹ã‚¯ãƒªãƒ—ãƒˆ (v2)
==============================================================

Geminiã®åˆ†æã‚’åæ˜ :
- render_modelã‚’ç‹¬ç«‹ã—ã¦ãƒ­ãƒ¼ãƒ‰
- æ˜ç¤ºçš„ã«ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ãƒ«ãƒ¼ãƒ—ã‚’å›ã—ã¦Refinerã‚’ç™ºå‹•
- TrackedData_inferã‹ã‚‰ã‚«ãƒ¡ãƒ©æƒ…å ±ã‚’å–å¾—

ä½¿ç”¨æ–¹æ³•:
  modal run extract_distillation_data.py
  modal run extract_distillation_data.py --num-frames 1000
"""

import modal
import os
import sys

# --- Volumeè¨­å®š ---
ehm_volume = modal.Volume.from_name("ehm-tracker-output")
weights_volume = modal.Volume.from_name("guava-weights")
distill_volume = modal.Volume.from_name("guava-distillation-data", create_if_missing=True)

# --- ç’°å¢ƒæ§‹æˆ (generate_ply_cloud.pyã¨åŒã˜) ---
image = (
    modal.Image.from_registry("nvidia/cuda:12.1.1-devel-ubuntu22.04", add_python="3.10")
    .apt_install(
        "libgl1-mesa-glx", "libglib2.0-0", "git", "ninja-build",
        "build-essential", "libglm-dev", "clang", "dos2unix", "ffmpeg",
        "libsm6", "libxext6", "libxrender-dev"
    )
    .env({
        "CUDA_HOME": "/usr/local/cuda",
        "MAX_JOBS": "4", "CC": "clang", "CXX": "clang++", "TORCH_CUDA_ARCH_LIST": "8.9"
    })
    .run_commands(
        "python -m pip install --upgrade pip setuptools wheel",
        "pip install \"numpy==1.26.4\" \"scipy\""
    )
    .run_commands("pip install chumpy --no-build-isolation")
    .run_commands("pip install \"torch==2.1.0\" \"torchvision==0.16.0\" --extra-index-url https://download.pytorch.org/whl/cu121")
    .pip_install(
        "lightning", "pytorch-lightning", "omegaconf", "gsplat",
        "opencv-python", "h5py", "tqdm", "scikit-image", "trimesh", "plyfile",
        "lmdb", "lpips", "open3d", "roma", "smplx", "yacs", "ninja",
        "colored", "termcolor", "tabulate", "vispy", "configargparse", "portalocker",
        "fvcore", "iopath", "imageio-ffmpeg"
    )
    .run_commands("pip install \"numpy==1.26.4\"")
    .run_commands("pip install --no-index --no-cache-dir pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/py310_cu121_pyt210/download.html")
    .add_local_dir("./submodules", remote_path="/root/GUAVA/submodules", copy=True)
    .run_commands(
        "cd /root/GUAVA/submodules/diff-gaussian-rasterization-32 && rm -rf build && pip install . --no-build-isolation",
        "cd /root/GUAVA/submodules/simple-knn && rm -rf build && pip install . --no-build-isolation"
    )
    .add_local_dir(".", remote_path="/root/GUAVA", copy=True, ignore=["assets/", "outputs/", ".venv/", ".git/"])
    .run_commands("find /root/GUAVA -maxdepth 3 -name '*.py' | xargs dos2unix")
)

app = modal.App("guava-distillation-extractor-v2")


def find_tracking_data(base_path, max_depth=3):
    """optim_tracking_ehm.pklã‚’æ¢ã™"""
    if max_depth <= 0:
        return None
    tracking_file = os.path.join(base_path, 'optim_tracking_ehm.pkl')
    if os.path.exists(tracking_file):
        return base_path
    if os.path.isdir(base_path):
        for item in os.listdir(base_path):
            item_path = os.path.join(base_path, item)
            if os.path.isdir(item_path):
                result = find_tracking_data(item_path, max_depth - 1)
                if result:
                    return result
    return None


@app.function(
    gpu="L4",
    image=image,
    volumes={
        "/root/EHM_results": ehm_volume,
        "/root/GUAVA/assets": weights_volume,
        "/root/distill_data": distill_volume
    },
    timeout=7200
)
def extract_distillation_data(
    output_name: str = "distill_dataset",
    num_frames: int = 1000,
    num_camera_angles: int = 5,  # ã‚«ãƒ¡ãƒ©ã‚¢ãƒ³ã‚°ãƒ«æ•°
):
    """
    GUAVAæ¨è«–+ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã‚’å®Ÿè¡Œã—ã¦è’¸ç•™ç”¨ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
    è¤‡æ•°ã‚«ãƒ¡ãƒ©ã‚¢ãƒ³ã‚°ãƒ«ã§ãƒ‡ãƒ¼ã‚¿ã‚’å¢—ã‚„ã™
    """
    import json
    import numpy as np
    from tqdm import tqdm
    import math
    
    os.chdir("/root/GUAVA")
    sys.path.insert(0, "/root/GUAVA")
    
    import torch
    import lightning
    from omegaconf import OmegaConf
    
    from dataset import TrackedData_infer
    from models.UbodyAvatar import Ubody_Gaussian_inferer, Ubody_Gaussian
    from models.UbodyAvatar.gaussian_render import GaussianRenderer
    from utils.general_utils import ConfigDict, find_pt_file, add_extra_cfgs
    
    print("=" * 70)
    print("ğŸ”¬ GUAVA Distillation Data Extractor v3 (Multi-Angle)")
    print("=" * 70)
    print(f"ğŸ“Š ç›®æ¨™ãƒ•ãƒ¬ãƒ¼ãƒ æ•°: {num_frames}")
    print(f"ğŸ“Š ã‚«ãƒ¡ãƒ©ã‚¢ãƒ³ã‚°ãƒ«æ•°: {num_camera_angles}")
    
    # --- ã‚«ãƒ¡ãƒ©ã‚¢ãƒ³ã‚°ãƒ«ç”Ÿæˆé–¢æ•° ---
    def generate_camera_angles(num_angles):
        """
        è¤‡æ•°ã®ã‚«ãƒ¡ãƒ©ã‚¢ãƒ³ã‚°ãƒ«ã‚’ç”Ÿæˆ
        æ­£é¢ + å·¦å³å›è»¢ + ä¸Šä¸‹å›è»¢
        """
        angles = []
        
        # æ­£é¢
        angles.append({'yaw': 0, 'pitch': 0, 'name': 'front'})
        
        if num_angles >= 3:
            # å·¦å³
            angles.append({'yaw': 15, 'pitch': 0, 'name': 'left_15'})
            angles.append({'yaw': -15, 'pitch': 0, 'name': 'right_15'})
        
        if num_angles >= 5:
            # ä¸Šä¸‹
            angles.append({'yaw': 0, 'pitch': 10, 'name': 'up_10'})
            angles.append({'yaw': 0, 'pitch': -10, 'name': 'down_10'})
        
        if num_angles >= 9:
            # æ–œã‚
            angles.append({'yaw': 10, 'pitch': 5, 'name': 'left_up'})
            angles.append({'yaw': -10, 'pitch': 5, 'name': 'right_up'})
            angles.append({'yaw': 10, 'pitch': -5, 'name': 'left_down'})
            angles.append({'yaw': -10, 'pitch': -5, 'name': 'right_down'})
        
        if num_angles >= 13:
            # ã‚ˆã‚Šå¤§ããªè§’åº¦
            angles.append({'yaw': 25, 'pitch': 0, 'name': 'left_25'})
            angles.append({'yaw': -25, 'pitch': 0, 'name': 'right_25'})
            angles.append({'yaw': 0, 'pitch': 15, 'name': 'up_15'})
            angles.append({'yaw': 0, 'pitch': -15, 'name': 'down_15'})
        
        return angles[:num_angles]
    
    def create_rotation_matrix(yaw_deg, pitch_deg, device):
        """
        Yaw (å·¦å³å›è»¢) ã¨ Pitch (ä¸Šä¸‹å›è»¢) ã‹ã‚‰å›è»¢è¡Œåˆ—ã‚’ç”Ÿæˆ
        """
        yaw = math.radians(yaw_deg)
        pitch = math.radians(pitch_deg)
        
        # Yaw (Yè»¸å›è»¢)
        cy, sy = math.cos(yaw), math.sin(yaw)
        Ry = torch.tensor([
            [cy, 0, sy],
            [0, 1, 0],
            [-sy, 0, cy]
        ], dtype=torch.float32, device=device)
        
        # Pitch (Xè»¸å›è»¢)
        cp, sp = math.cos(pitch), math.sin(pitch)
        Rx = torch.tensor([
            [1, 0, 0],
            [0, cp, -sp],
            [0, sp, cp]
        ], dtype=torch.float32, device=device)
        
        # åˆæˆ
        R = Ry @ Rx
        return R
    
    def apply_camera_rotation(base_w2c, yaw_deg, pitch_deg, device):
        """
        åŸºæœ¬ã®world-to-cameraè¡Œåˆ—ã«å›è»¢ã‚’é©ç”¨
        """
        if base_w2c.dim() == 2:
            base_w2c = base_w2c.unsqueeze(0)
        
        batch_size = base_w2c.shape[0]
        R_delta = create_rotation_matrix(yaw_deg, pitch_deg, device)
        
        # 4x4è¡Œåˆ—ã«æ‹¡å¼µ
        R_delta_4x4 = torch.eye(4, device=device)
        R_delta_4x4[:3, :3] = R_delta
        
        # å›è»¢ã‚’é©ç”¨
        new_w2c = R_delta_4x4.unsqueeze(0) @ base_w2c
        
        return new_w2c
    
    # --- Assetsç¢ºèª ---
    print("\n--- Checking Assets ---")
    if not (os.path.exists("assets") and os.path.exists("assets/GUAVA")):
        print("âŒ Assets folder not found!")
        return None
    print(f"âœ… GUAVA assets: {os.listdir('assets/GUAVA')}")
    
    # --- ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹æ¢ç´¢ ---
    search_path = "/root/EHM_results/processed_data"
    data_path = find_tracking_data(search_path) if os.path.exists(search_path) else None
    
    if not data_path:
        print(f"âŒ Tracking data not found in {search_path}")
        return None
    print(f"âœ… ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹: {data_path}")
    
    # --- ãƒ¢ãƒ‡ãƒ«è¨­å®š ---
    model_path = "assets/GUAVA"
    model_config_path = os.path.join(model_path, 'config.yaml')
    
    meta_cfg = ConfigDict(model_config_path=model_config_path)
    meta_cfg = add_extra_cfgs(meta_cfg)
    
    lightning.fabric.seed_everything(10)
    device = 'cuda:0'
    
    # --- ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ­ãƒ¼ãƒ‰ ---
    ckpt_path = os.path.join(model_path, 'checkpoints')
    base_model = find_pt_file(ckpt_path, 'best') or find_pt_file(ckpt_path, 'latest')
    
    if not base_model or not os.path.exists(base_model):
        print(f"âŒ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {ckpt_path}")
        return None
    
    print(f"\nğŸ”„ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­ã¿è¾¼ã¿: {base_model}")
    state = torch.load(base_model, map_location='cpu', weights_only=True)
    
    # --- Infer Modelèª­ã¿è¾¼ã¿ ---
    print("\nğŸ”„ Infer Modelèª­ã¿è¾¼ã¿ä¸­...")
    infer_model = Ubody_Gaussian_inferer(meta_cfg.MODEL)
    infer_model.to(device)
    infer_model.eval()
    infer_model.load_state_dict(state['model'], strict=False)
    print(f"âœ… Infer Modelèª­ã¿è¾¼ã¿å®Œäº†")
    
    # --- Render Modelèª­ã¿è¾¼ã¿ ---
    print("\nğŸ”„ Render Modelèª­ã¿è¾¼ã¿ä¸­...")
    render_model = GaussianRenderer(meta_cfg.MODEL)
    render_model.to(device)
    render_model.eval()
    
    # Neural Refinerã®é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰
    if 'render_model' in state:
        print(f"  Found 'render_model' in checkpoint")
        render_model.load_state_dict(state['render_model'], strict=False)
        print(f"âœ… Render Model weights loaded from state['render_model']")
    else:
        print("âš ï¸ No render_model found in checkpoint")
    
    print(f"âœ… Render Modelèª­ã¿è¾¼ã¿å®Œäº†")
    
    # --- Hookè¨­å®š ---
    captured_data = {"input": None, "output": None}
    
    def refiner_hook(module, input, output):
        captured_data["input"] = input[0].detach().cpu()
        captured_data["output"] = output.detach().cpu()
    
    render_model.nerual_refiner.register_forward_hook(refiner_hook)
    print("âœ… Hook registered on neural_refiner")
    
    # --- å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª ---
    output_dir = os.path.join("/root/distill_data", output_name)
    os.makedirs(os.path.join(output_dir, 'features'), exist_ok=True)
    os.makedirs(os.path.join(output_dir, 'rgb'), exist_ok=True)
    
    # --- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š ---
    meta_cfg['DATASET']['data_path'] = data_path
    OmegaConf.set_readonly(meta_cfg._dot_config, False)
    meta_cfg._dot_config.DATASET.data_path = data_path
    OmegaConf.set_readonly(meta_cfg._dot_config, True)
    
    test_dataset = TrackedData_infer(cfg=meta_cfg, split='test', device=device, test_full=True)
    print(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿å®Œäº†: {len(test_dataset)} ã‚µãƒ³ãƒ—ãƒ«")
    
    video_ids = list(test_dataset.videos_info.keys())
    print(f"  video_ids: {video_ids}")
    
    # --- ã‚«ãƒ¡ãƒ©ã‚¢ãƒ³ã‚°ãƒ«ç”Ÿæˆ ---
    camera_angles = generate_camera_angles(num_camera_angles)
    print(f"\nğŸ“· ã‚«ãƒ¡ãƒ©ã‚¢ãƒ³ã‚°ãƒ«: {[a['name'] for a in camera_angles]}")
    
    # --- ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºãƒ«ãƒ¼ãƒ— ---
    print("\n" + "=" * 70)
    print("ğŸš€ ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºé–‹å§‹ (Multi-Angle)")
    print("=" * 70)
    
    saved_count = 0
    
    with torch.no_grad():
        for vidx, video_id in enumerate(video_ids):
            if saved_count >= num_frames:
                print(f"\nâœ… ç›®æ¨™ãƒ•ãƒ¬ãƒ¼ãƒ æ•° ({num_frames}) ã«åˆ°é”")
                break
            
            print(f"\n--- å‹•ç”»: {video_id} [{vidx+1}/{len(video_ids)}] ---")
            
            # ã‚½ãƒ¼ã‚¹æƒ…å ±èª­ã¿è¾¼ã¿
            source_info = test_dataset._load_source_info(video_id)
            
            # Gaussianæ¨è«–
            print("  ğŸ”„ Gaussianæ¨è«–ä¸­...")
            vertex_gs_dict, up_point_gs_dict, extra_dict = infer_model(source_info)
            print(f"  âœ… Gaussianæ¨è«–å®Œäº†")
            
            # Ubody Gaussianä½œæˆ
            ubody_gaussians = Ubody_Gaussian(
                meta_cfg.MODEL,
                vertex_gs_dict,
                up_point_gs_dict,
                pruning=True
            )
            ubody_gaussians.init_ehm(infer_model.ehm)
            ubody_gaussians.eval()
            
            # Canonical Gaussiansã‚’å–å¾—
            if not ubody_gaussians._canoical:
                ubody_gaussians.get_canoical_gaussians()
            print(f"  âœ… Canonical Gaussianså–å¾—å®Œäº†")
            
            # Gaussian assetsã‚’æ§‹ç¯‰ (å…¨ãƒ•ãƒ¬ãƒ¼ãƒ å…±é€š)
            xyz_all = torch.cat([ubody_gaussians._smplx_xyz, ubody_gaussians._uv_xyz_cano], dim=1)
            opacity_all = torch.cat([ubody_gaussians._smplx_opacity, ubody_gaussians._uv_opacity_cano], dim=1)
            scaling_all = torch.cat([ubody_gaussians._smplx_scaling, ubody_gaussians._uv_scaling_cano], dim=1)
            rotation_all = torch.cat([ubody_gaussians._smplx_rotation, ubody_gaussians._uv_rotation_cano], dim=1)
            features_all = torch.cat([ubody_gaussians._smplx_features_color, ubody_gaussians._uv_features_color], dim=1)
            
            gaussian_assets = {
                'xyz': xyz_all.to(device),
                'opacity': opacity_all.to(device),
                'scaling': scaling_all.to(device),
                'rotation': rotation_all.to(device),
                'features_color': features_all.to(device),
            }
            
            dataset_len = len(test_dataset)
            total_iterations = dataset_len * len(camera_angles)
            
            print(f"  ğŸ“Š ãƒ•ãƒ¬ãƒ¼ãƒ æ•°: {dataset_len}, ã‚¢ãƒ³ã‚°ãƒ«æ•°: {len(camera_angles)}, åˆè¨ˆ: {total_iterations}")
            
            # ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ã‚¤ãƒ†ãƒ¬ãƒ¼ãƒˆ
            pbar = tqdm(range(dataset_len), desc=f"  Rendering")
            for idx in pbar:
                if saved_count >= num_frames:
                    break
                
                try:
                    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰ãƒãƒƒãƒã‚’å–å¾—
                    batch = test_dataset[idx]
                    
                    if batch is None:
                        continue
                    
                    # ãƒ™ãƒ¼ã‚¹ã®ã‚«ãƒ¡ãƒ©è¡Œåˆ—ã‚’å–å¾—
                    if hasattr(batch, 'keys') and 'w2c_cam' in batch.get('source', {}):
                        base_w2c = batch['source']['w2c_cam']
                    elif hasattr(batch, 'keys') and 'target' in batch and 'w2c_cam' in batch['target']:
                        base_w2c = batch['target']['w2c_cam']
                    else:
                        base_w2c = torch.eye(4, device=device)
                    
                    if base_w2c.dim() == 2:
                        base_w2c = base_w2c.unsqueeze(0)
                    base_w2c = base_w2c.to(device)
                    
                    # å„ã‚«ãƒ¡ãƒ©ã‚¢ãƒ³ã‚°ãƒ«ã§ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°
                    for angle in camera_angles:
                        if saved_count >= num_frames:
                            break
                        
                        # ã‚«ãƒ¡ãƒ©å›è»¢ã‚’é©ç”¨
                        w2c = apply_camera_rotation(base_w2c, angle['yaw'], angle['pitch'], device)
                        
                        cam_params = {
                            'image_height': torch.tensor([512], device=device),
                            'image_width': torch.tensor([512], device=device),
                            'tanfovx': torch.tensor([meta_cfg.MODEL.invtanfov], device=device),
                            'tanfovy': torch.tensor([meta_cfg.MODEL.invtanfov], device=device),
                            'world_view_transform': w2c,
                            'full_proj_transform': w2c,
                            'camera_center': torch.zeros(1, 3, device=device),
                        }
                        
                        # ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°å®Ÿè¡Œ
                        render_output = render_model(
                            gaussian_assets,
                            cam_params,
                            bg=1.0
                        )
                        
                        # Hookã§ã‚­ãƒ£ãƒ—ãƒãƒ£ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
                        if captured_data["input"] is not None and captured_data["output"] is not None:
                            # ä¸æ­£ãªã‚µãƒ³ãƒ—ãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—
                            input_data = captured_data["input"][0]
                            
                            # åˆ†æ•£ãƒã‚§ãƒƒã‚¯ (èƒŒæ™¯ã®ã¿ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—)
                            if input_data.std() < 0.01:
                                print(f"    âš ï¸ Skipping sample (low variance: {input_data.std():.4f})")
                                captured_data["input"] = None
                                captured_data["output"] = None
                                continue
                            
                            # NaNãƒã‚§ãƒƒã‚¯
                            if torch.isnan(input_data).any():
                                print(f"    âš ï¸ Skipping sample (contains NaN)")
                                captured_data["input"] = None
                                captured_data["output"] = None
                                continue
                            
                            save_id = f"{saved_count:06d}"
                            
                            # 32chç‰¹å¾´ãƒãƒƒãƒ—ã‚’ä¿å­˜
                            feat_path = os.path.join(output_dir, 'features', f'{save_id}.pt')
                            torch.save(input_data.clone(), feat_path)
                            
                            # RGBå‡ºåŠ›ã‚’ä¿å­˜
                            rgb_path = os.path.join(output_dir, 'rgb', f'{save_id}.pt')
                            torch.save(captured_data["output"][0].clone(), rgb_path)
                            
                            saved_count += 1
                            
                            # ãƒªã‚»ãƒƒãƒˆ
                            captured_data["input"] = None
                            captured_data["output"] = None
                        
                        pbar.set_postfix({'saved': saved_count, 'angle': angle['name']})
                    
                except Exception as e:
                    if idx < 3:
                        import traceback
                        print(f"    âŒ Frame {idx} error: {type(e).__name__}: {e}")
                        traceback.print_exc()
                    continue
            
            print(f"\n  ğŸ“Š ä¿å­˜æ¸ˆã¿: {saved_count} ãƒšã‚¢")
    
    # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    if hasattr(test_dataset, '_lmdb_engine') and test_dataset._lmdb_engine:
        test_dataset._lmdb_engine.close()
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜
    metadata = {
        'num_samples': saved_count,
        'feature_shape': [32, 512, 512],
        'rgb_shape': [3, 512, 512],
        'source': 'GUAVA Neural Refiner',
        'teacher_model': 'StyleUNet (107MB)',
        'camera_angles': [a['name'] for a in camera_angles],
        'num_camera_angles': len(camera_angles)
    }
    torch.save(metadata, os.path.join(output_dir, 'metadata.pt'))
    
    # çµ±è¨ˆä¿å­˜
    stats = {
        'total_frames': saved_count,
        'saved_pairs': saved_count,
        'save_dir': output_dir,
        'video_ids': video_ids,
        'camera_angles': [a['name'] for a in camera_angles]
    }
    with open(os.path.join(output_dir, 'extraction_stats.json'), 'w') as f:
        json.dump(stats, f, indent=2)
    
    print("\n" + "=" * 70)
    print("ğŸ‰ ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºå®Œäº†!")
    print("=" * 70)
    print(f"ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {output_dir}")
    print(f"ğŸ“Š æŠ½å‡ºãƒ•ãƒ¬ãƒ¼ãƒ æ•°: {saved_count}")
    print(f"ğŸ“· ã‚«ãƒ¡ãƒ©ã‚¢ãƒ³ã‚°ãƒ«: {[a['name'] for a in camera_angles]}")
    
    # Volume ã‚³ãƒŸãƒƒãƒˆ
    distill_volume.commit()
    print("\nâœ… Data committed to Volume")
    
    return stats


@app.function(
    image=image,
    volumes={"/root/distill_data": distill_volume},
)
def verify_extracted_data(dataset_name: str = "distill_dataset"):
    """æŠ½å‡ºã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’æ¤œè¨¼"""
    import torch
    
    data_dir = f"/root/distill_data/{dataset_name}"
    
    if not os.path.exists(data_dir):
        print(f"âŒ Dataset not found: {data_dir}")
        return None
    
    features_dir = os.path.join(data_dir, 'features')
    rgb_dir = os.path.join(data_dir, 'rgb')
    
    feature_files = sorted([f for f in os.listdir(features_dir) if f.endswith('.pt')])
    rgb_files = sorted([f for f in os.listdir(rgb_dir) if f.endswith('.pt')])
    
    print(f"ğŸ“Š Features: {len(feature_files)} files")
    print(f"ğŸ“Š RGB: {len(rgb_files)} files")
    
    if len(feature_files) == 0:
        print("âŒ No data found")
        return None
    
    # ã‚µãƒ³ãƒ—ãƒ«æ¤œè¨¼
    print("\n--- Sample Verification ---")
    for i in range(min(3, len(feature_files))):
        feat = torch.load(os.path.join(features_dir, feature_files[i]))
        rgb = torch.load(os.path.join(rgb_dir, rgb_files[i]))
        
        print(f"\nSample {i}:")
        print(f"  Feature shape: {feat.shape}")
        print(f"  Feature range: [{feat.min():.3f}, {feat.max():.3f}]")
        print(f"  RGB shape: {rgb.shape}")
        print(f"  RGB range: [{rgb.min():.3f}, {rgb.max():.3f}]")
    
    return {
        'num_samples': len(feature_files),
        'feature_shape': list(feat.shape),
        'rgb_shape': list(rgb.shape)
    }


@app.local_entrypoint()
def main(
    action: str = "extract",
    output_name: str = "distill_dataset",
    num_frames: int = 1000,
    num_angles: int = 5,
):
    """
    ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ
    
    ä½¿ç”¨æ–¹æ³•:
        modal run extract_distillation_data.py --action extract --num-frames 1000 --num-angles 5
        modal run extract_distillation_data.py --action verify
    """
    
    if action == "extract":
        print("ğŸš€ Starting distillation data extraction...")
        print(f"ğŸ“· Camera angles: {num_angles}")
        result = extract_distillation_data.remote(
            output_name=output_name,
            num_frames=num_frames,
            num_camera_angles=num_angles,
        )
        print(f"\nğŸ“Š Result: {result}")
        
    elif action == "verify":
        print("ğŸ” Verifying extracted data...")
        result = verify_extracted_data.remote(output_name)
        print(f"\nğŸ“Š Result: {result}")
        
    else:
        print(f"Unknown action: {action}")
        print("Available actions: extract, verify")
