"""
GUAVA PLY File Generator - ä¸‰è§’å½¢ãƒ‡ãƒ¼ã‚¿è¿½åŠ ç‰ˆ
==========================================
å…¬å¼save_gaussian_ply()ã®å‡ºåŠ›ã«ã€EHMãƒ¡ãƒƒã‚·ãƒ¥ã®ä¸‰è§’å½¢ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 

ä½¿ç”¨æ–¹æ³•:
  modal run generate_ply_cloud.py
  modal run generate_ply_cloud.py --output-name my_avatar
"""

import modal
import os
import sys

# --- 1. Volumeè¨­å®š ---
ehm_volume = modal.Volume.from_name("ehm-tracker-output")
weights_volume = modal.Volume.from_name("guava-weights")
ply_output_volume = modal.Volume.from_name("guava-ply-output", create_if_missing=True)

# --- 2. ç’°å¢ƒæ§‹æˆ ---
image = (
    modal.Image.from_registry("nvidia/cuda:12.1.1-devel-ubuntu22.04", add_python="3.10")
    .apt_install(
        "libgl1-mesa-glx", "libglib2.0-0", "git", "ninja-build",
        "build-essential", "libglm-dev", "clang", "dos2unix", "ffmpeg",
        "libsm6", "libxext6", "libxrender-dev"
    )
    .env({
        "CUDA_HOME": "/usr/local/cuda",
        "MAX_JOBS": "4", "CC": "clang", "CXX": "clang++", "TORCH_CUDA_ARCH_LIST": "8.9"
    })
    .run_commands(
        "python -m pip install --upgrade pip setuptools wheel",
        "pip install \"numpy==1.26.4\" \"scipy\""
    )
    .run_commands("pip install chumpy --no-build-isolation")
    .run_commands("pip install \"torch==2.1.0\" \"torchvision==0.16.0\" --extra-index-url https://download.pytorch.org/whl/cu121")
    .pip_install(
        "lightning", "pytorch-lightning", "omegaconf", "gsplat",
        "opencv-python", "h5py", "tqdm", "scikit-image", "trimesh", "plyfile",
        "lmdb", "lpips", "open3d", "roma", "smplx", "yacs", "ninja",
        "colored", "termcolor", "tabulate", "vispy", "configargparse", "portalocker",
        "fvcore", "iopath", "imageio-ffmpeg"
    )
    .run_commands("pip install \"numpy==1.26.4\"")
    .run_commands("pip install --no-index --no-cache-dir pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/py310_cu121_pyt210/download.html")
    .add_local_dir("./submodules", remote_path="/root/GUAVA/submodules", copy=True)
    .run_commands(
        "cd /root/GUAVA/submodules/diff-gaussian-rasterization-32 && rm -rf build && pip install . --no-build-isolation",
        "cd /root/GUAVA/submodules/simple-knn && rm -rf build && pip install . --no-build-isolation"
    )
    .add_local_dir(".", remote_path="/root/GUAVA", copy=True, ignore=["assets/", "outputs/", ".venv/", ".git/"])
    .run_commands("find /root/GUAVA -maxdepth 3 -name '*.py' | xargs dos2unix")
)

app = modal.App("guava-ply-generator-with-faces")


def add_faces_to_ply(source_ply_path, output_ply_path, faces):
    """
    æ—¢å­˜ã®PLYãƒ•ã‚¡ã‚¤ãƒ«ã«ä¸‰è§’å½¢ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
    
    Args:
        source_ply_path: å…ƒã®PLYãƒ•ã‚¡ã‚¤ãƒ« (é ‚ç‚¹ã®ã¿)
        output_ply_path: å‡ºåŠ›PLYãƒ•ã‚¡ã‚¤ãƒ« (é ‚ç‚¹+ä¸‰è§’å½¢)
        faces: ä¸‰è§’å½¢ãƒ‡ãƒ¼ã‚¿ (numpy array, shape: [num_faces, 3])
    """
    import struct
    
    # Step 1: å…ƒã®PLYãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
    with open(source_ply_path, 'rb') as f:
        header_bytes = f.read(10000)
        header_text = header_bytes.decode('ascii', errors='ignore')
        end_header_pos = header_text.find('end_header')
        
        if end_header_pos == -1:
            raise ValueError("Invalid PLY file: no end_header found")
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼è§£æ
        header_lines = header_text[:end_header_pos].split('\n')
        vertex_count = 0
        properties = []
        
        for line in header_lines:
            line = line.strip()
            if line.startswith('element vertex'):
                vertex_count = int(line.split()[2])
            elif line.startswith('property'):
                parts = line.split()
                if len(parts) >= 3:
                    properties.append(line)  # ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£è¡Œå…¨ä½“ã‚’ä¿å­˜
        
        # é ‚ç‚¹ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        header_size = end_header_pos + len('end_header\n')
        f.seek(header_size)
        vertex_data = f.read()
    
    # Step 2: æ–°ã—ã„ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ä½œæˆï¼ˆä¸‰è§’å½¢ãƒ‡ãƒ¼ã‚¿è¿½åŠ ï¼‰
    num_faces = faces.shape[0]
    
    new_header = "ply\n"
    new_header += "format binary_little_endian 1.0\n"
    new_header += f"element vertex {vertex_count}\n"
    
    # ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ä¸€è¦§ã‚’è¿½åŠ 
    for prop in properties:
        new_header += prop + "\n"
    
    # ä¸‰è§’å½¢è¦ç´ ã‚’è¿½åŠ 
    new_header += f"element face {num_faces}\n"
    new_header += "property list uchar uint vertex_indices\n"
    new_header += "end_header\n"
    
    # Step 3: æ–°ã—ã„PLYãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
    with open(output_ply_path, 'wb') as f:
        # ãƒ˜ãƒƒãƒ€ãƒ¼æ›¸ãè¾¼ã¿
        f.write(new_header.encode('ascii'))
        
        # é ‚ç‚¹ãƒ‡ãƒ¼ã‚¿æ›¸ãè¾¼ã¿ï¼ˆæ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ãã®ã¾ã¾ï¼‰
        f.write(vertex_data)
        
        # ä¸‰è§’å½¢ãƒ‡ãƒ¼ã‚¿æ›¸ãè¾¼ã¿
        for i in range(num_faces):
            f.write(struct.pack('<B', 3))  # 3é ‚ç‚¹ã®ä¸‰è§’å½¢
            f.write(struct.pack('<III', 
                int(faces[i, 0]), 
                int(faces[i, 1]), 
                int(faces[i, 2])
            ))
    
    return output_ply_path


def save_web_compatible_ply(ubody_gaussians, output_dir, faces=None):
    """
    gvrm.ts/ply.tsäº’æ›ã®PLYãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜

    ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ (å…¨ã¦float32):
    - x, y, z (ä½ç½®)
    - nx, ny, nz (æ³•ç·š)
    - f_dc_0, f_dc_1, f_dc_2 (SHå½¢å¼ã®è‰²)
    - scale_0, scale_1, scale_2 (ã‚¹ã‚±ãƒ¼ãƒ«)

    ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ä¸‰è§’å½¢ãƒ‡ãƒ¼ã‚¿ã‚‚è¿½åŠ 
    """
    import numpy as np
    import struct
    import os

    # Canonical Gaussiansã‚’å–å¾—
    if not ubody_gaussians._canoical:
        ubody_gaussians.get_canoical_gaussians()

    # ãƒ‡ãƒ¼ã‚¿å–å¾—
    xyz = ubody_gaussians._xyz.detach().cpu().numpy()  # [N, 3]
    features_dc = ubody_gaussians._features_dc.detach().cpu().numpy()  # [N, 1, 3]
    scaling = ubody_gaussians._scaling.detach().cpu().numpy()  # [N, 3]

    num_points = xyz.shape[0]

    # æ³•ç·šï¼ˆã‚¼ãƒ­ã§åˆæœŸåŒ– - å¾Œã§ãƒ¡ãƒƒã‚·ãƒ¥ã‹ã‚‰è¨ˆç®—å¯èƒ½ï¼‰
    normals = np.zeros((num_points, 3), dtype=np.float32)

    # SHä¿‚æ•°ã‚’å–ã‚Šå‡ºã—
    f_dc = features_dc[:, 0, :]  # [N, 3]

    # PLYãƒ˜ãƒƒãƒ€ãƒ¼ä½œæˆ
    output_path = os.path.join(output_dir, 'avatar_web.ply')

    header = "ply\n"
    header += "format binary_little_endian 1.0\n"
    header += f"element vertex {num_points}\n"
    header += "property float x\n"
    header += "property float y\n"
    header += "property float z\n"
    header += "property float nx\n"
    header += "property float ny\n"
    header += "property float nz\n"
    header += "property float f_dc_0\n"
    header += "property float f_dc_1\n"
    header += "property float f_dc_2\n"
    header += "property float scale_0\n"
    header += "property float scale_1\n"
    header += "property float scale_2\n"

    # ä¸‰è§’å½¢ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Œã°è¿½åŠ 
    if faces is not None:
        num_faces = faces.shape[0]
        header += f"element face {num_faces}\n"
        header += "property list uchar uint vertex_indices\n"

    header += "end_header\n"

    # ãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿ä½œæˆ
    with open(output_path, 'wb') as f:
        f.write(header.encode('ascii'))

        # é ‚ç‚¹ãƒ‡ãƒ¼ã‚¿æ›¸ãè¾¼ã¿
        for i in range(num_points):
            # x, y, z
            f.write(struct.pack('<fff', xyz[i, 0], xyz[i, 1], xyz[i, 2]))
            # nx, ny, nz
            f.write(struct.pack('<fff', normals[i, 0], normals[i, 1], normals[i, 2]))
            # f_dc_0, f_dc_1, f_dc_2
            f.write(struct.pack('<fff', f_dc[i, 0], f_dc[i, 1], f_dc[i, 2]))
            # scale_0, scale_1, scale_2
            f.write(struct.pack('<fff', scaling[i, 0], scaling[i, 1], scaling[i, 2]))

        # ä¸‰è§’å½¢ãƒ‡ãƒ¼ã‚¿æ›¸ãè¾¼ã¿
        if faces is not None:
            for i in range(num_faces):
                f.write(struct.pack('<B', 3))
                f.write(struct.pack('<III',
                    int(faces[i, 0]),
                    int(faces[i, 1]),
                    int(faces[i, 2])
                ))

    return output_path, num_points


def verify_ply_format(ply_path):
    """PLYãƒ•ã‚¡ã‚¤ãƒ«ã®å½¢å¼ã‚’æ¤œè¨¼"""
    import struct
    
    try:
        with open(ply_path, 'rb') as f:
            header_bytes = f.read(10000)
            header_text = header_bytes.decode('ascii', errors='ignore')
            end_header_pos = header_text.find('end_header')
            
            if end_header_pos == -1:
                return {"error": "Invalid PLY: no end_header"}
            
            header_lines = header_text[:end_header_pos].split('\n')
            
            vertex_count = 0
            face_count = 0
            properties = []
            in_vertex_section = False
            
            for line in header_lines:
                line = line.strip()
                
                if line.startswith('element vertex'):
                    vertex_count = int(line.split()[2])
                    in_vertex_section = True
                elif line.startswith('element face'):
                    face_count = int(line.split()[2])
                    in_vertex_section = False
                elif line.startswith('property') and in_vertex_section:
                    parts = line.split()
                    if len(parts) >= 3:
                        properties.append(parts[-1])
            
            file_size = os.path.getsize(ply_path)
            header_size = end_header_pos + len('end_header\n')
            vertex_data_size = vertex_count * len(properties) * 4
            face_data_size = face_count * (1 + 3 * 4) if face_count > 0 else 0
            expected_size = header_size + vertex_data_size + face_data_size
            actual_data_size = file_size - header_size
            
            return {
                "vertex_count": vertex_count,
                "face_count": face_count,
                "property_count": len(properties),
                "properties": properties,
                "file_size_mb": file_size / (1024 * 1024),
                "header_size": header_size,
                "expected_data_size": vertex_data_size + face_data_size,
                "actual_data_size": actual_data_size,
                "size_match": abs(actual_data_size - (vertex_data_size + face_data_size)) < 1000,
                "has_faces": face_count > 0,
                "has_opacity": 'opacity' in properties,
                "has_rotation": 'rot_0' in properties,
                "has_scale": 'scale_0' in properties,
            }
    except Exception as e:
        return {"error": str(e)}


def extract_ply_sample(ply_path, num_samples=5):
    """PLYãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æœ€åˆã®æ•°é ‚ç‚¹ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
    import struct
    
    try:
        with open(ply_path, 'rb') as f:
            header_bytes = f.read(10000)
            header_text = header_bytes.decode('ascii', errors='ignore')
            end_header_pos = header_text.find('end_header')
            
            header_lines = header_text[:end_header_pos].split('\n')
            
            vertex_count = 0
            properties = []
            in_vertex_section = False
            
            for line in header_lines:
                line = line.strip()
                if line.startswith('element vertex'):
                    vertex_count = int(line.split()[2])
                    in_vertex_section = True
                elif line.startswith('element face'):
                    in_vertex_section = False
                elif line.startswith('property') and in_vertex_section:
                    parts = line.split()
                    if len(parts) >= 3:
                        properties.append(parts[-1])
            
            header_size = end_header_pos + len('end_header\n')
            f.seek(header_size)
            
            stride = len(properties) * 4
            
            print(f"\n    ğŸ“Š First {num_samples} vertices:")
            for i in range(min(num_samples, vertex_count)):
                vertex_data = f.read(stride)
                values = struct.unpack('<' + 'f' * len(properties), vertex_data)
                
                print(f"      Vertex {i}:")
                for j, prop in enumerate(properties[:12]):
                    print(f"        {prop:12s} = {values[j]:10.6f}")
                
                if i < num_samples - 1:
                    print()
                
    except Exception as e:
        print(f"    âŒ Sample extraction failed: {e}")


@app.function(
    gpu="L4",
    image=image,
    volumes={
        "/root/EHM_results": ehm_volume,
        "/root/GUAVA/assets": weights_volume,
        "/root/GUAVA/ply_outputs": ply_output_volume
    },
    timeout=3600
)
def generate_ply(
    output_name: str = "guava_avatar",
    save_split: bool = False,
    save_point_cloud: bool = False,
    save_gaussian: bool = True,
    save_web: bool = True,
    verify_format: bool = True,
    extract_samples: bool = True
):
    """
    è«–æ–‡æº–æ‹ ã®å®Œå…¨ãªPLYãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆï¼ˆä¸‰è§’å½¢ãƒ‡ãƒ¼ã‚¿ä»˜ãï¼‰

    Args:
        output_name: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå
        save_split: SMPLX/UVåˆ¥ã€…ã«ä¿å­˜ã™ã‚‹ã‹
        save_point_cloud: Open3Då½¢å¼ã®ç‚¹ç¾¤PLYã‚’ä¿å­˜ã™ã‚‹ã‹
        save_gaussian: 3DGSå½¢å¼ã®PLYã‚’ä¿å­˜ã™ã‚‹ã‹ï¼ˆè«–æ–‡æº–æ‹ ï¼‰
        save_web: gvrm.ts/ply.tsäº’æ›ã®PLYã‚’ä¿å­˜ã™ã‚‹ã‹
        verify_format: PLYãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã‚’æ¤œè¨¼ã™ã‚‹ã‹
        extract_samples: ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã™ã‚‹ã‹
    """
    import json
    import numpy as np

    os.chdir("/root/GUAVA")
    sys.path.insert(0, "/root/GUAVA")

    import torch
    import lightning
    from omegaconf import OmegaConf

    from dataset import TrackedData_infer
    from models.UbodyAvatar import Ubody_Gaussian_inferer, Ubody_Gaussian
    from utils.general_utils import ConfigDict, find_pt_file, add_extra_cfgs

    print("=" * 70)
    print("GUAVA PLY Generator (ä¸‰è§’å½¢ãƒ‡ãƒ¼ã‚¿è¿½åŠ ç‰ˆ)")
    print("å…¬å¼PLY + EHMãƒ¡ãƒƒã‚·ãƒ¥ã®ä¸‰è§’å½¢ãƒ‡ãƒ¼ã‚¿")
    print("=" * 70)

    # --- Assetsç¢ºèª ---
    print("\n--- Checking Assets ---")
    if os.path.exists("assets") and os.path.exists("assets/GUAVA"):
        print(f"âœ… GUAVA assets: {os.listdir('assets/GUAVA')}")
    else:
        print("âŒ Assets folder not found!")
        return None

    # --- ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹æ¢ç´¢ ---
    search_path = "/root/EHM_results/processed_data"
    
    def find_tracking_data(base_path, max_depth=3):
        if max_depth <= 0:
            return None
        tracking_file = os.path.join(base_path, 'optim_tracking_ehm.pkl')
        if os.path.exists(tracking_file):
            return base_path
        if os.path.isdir(base_path):
            for item in os.listdir(base_path):
                item_path = os.path.join(base_path, item)
                if os.path.isdir(item_path):
                    result = find_tracking_data(item_path, max_depth - 1)
                    if result:
                        return result
        return None

    data_path = None
    if os.path.exists(search_path):
        data_path = find_tracking_data(search_path)
        if data_path:
            print(f"âœ… ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹: {data_path}")
        else:
            print(f"âŒ optim_tracking_ehm.pkl ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return None
    else:
        print(f"âŒ EHMçµæœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {search_path}")
        return None

    print(f"\nğŸ“ å‡ºåŠ›å: {output_name}")
    print("=" * 70)

    # --- ãƒ¢ãƒ‡ãƒ«è¨­å®š ---
    model_path = "assets/GUAVA"
    model_config_path = os.path.join(model_path, 'config.yaml')

    meta_cfg = ConfigDict(model_config_path=model_config_path)
    meta_cfg = add_extra_cfgs(meta_cfg)

    lightning.fabric.seed_everything(10)
    device = 'cuda:0'

    # --- ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ ---
    print("\nğŸ”„ ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    infer_model = Ubody_Gaussian_inferer(meta_cfg.MODEL)
    infer_model.to(device)
    infer_model.eval()

    ckpt_path = os.path.join(model_path, 'checkpoints')
    base_model = find_pt_file(ckpt_path, 'best')
    if base_model is None:
        base_model = find_pt_file(ckpt_path, 'latest')

    if base_model is None or not os.path.exists(base_model):
        print(f"âŒ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {ckpt_path}")
        return None

    _state = torch.load(base_model, map_location='cpu', weights_only=True)
    infer_model.load_state_dict(_state['model'], strict=False)
    print(f"âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")

    # --- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š ---
    meta_cfg['DATASET']['data_path'] = data_path
    OmegaConf.set_readonly(meta_cfg._dot_config, False)
    meta_cfg._dot_config.DATASET.data_path = data_path
    OmegaConf.set_readonly(meta_cfg._dot_config, True)

    test_dataset = TrackedData_infer(cfg=meta_cfg, split='test', device=device, test_full=True)
    print(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿å®Œäº†: {len(test_dataset)} ã‚µãƒ³ãƒ—ãƒ«")

    # --- å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª ---
    output_dir = os.path.join("/root/GUAVA/ply_outputs", output_name)
    os.makedirs(output_dir, exist_ok=True)

    video_ids = list(test_dataset.videos_info.keys())
    results = []

    with torch.no_grad():
        for vidx, video_id in enumerate(video_ids):
            print(f"\n{'='*70}")
            print(f"å‡¦ç†ä¸­: {video_id} [{vidx+1}/{len(video_ids)}]")
            print(f"{'='*70}")

            video_output_dir = os.path.join(output_dir, video_id)
            os.makedirs(video_output_dir, exist_ok=True)

            # ã‚½ãƒ¼ã‚¹æƒ…å ±èª­ã¿è¾¼ã¿
            source_info = test_dataset._load_source_info(video_id)

            # Ubody Gaussiansæ¨è«–
            print("  ğŸ”„ Ubody Gaussiansæ¨è«–ä¸­...")
            import time
            start_time = time.time()
            vertex_gs_dict, up_point_gs_dict, extra_dict = infer_model(source_info)
            infer_time = time.time() - start_time

            # Ubody Gaussian ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä½œæˆ
            ubody_gaussians = Ubody_Gaussian(
                meta_cfg.MODEL,
                vertex_gs_dict,
                up_point_gs_dict,
                pruning=True
            )
            ubody_gaussians.init_ehm(infer_model.ehm)
            ubody_gaussians.eval()

            # PLYãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
            ply_files = []

            if save_point_cloud:
                print("  ğŸ’¾ ç‚¹ç¾¤PLYã‚’ä¿å­˜ä¸­...")
                ubody_gaussians.save_point_ply(video_output_dir, save_split=save_split)
                ply_files.append('canonical.ply')
                if save_split:
                    ply_files.extend(['canonical_smplx.ply', 'canonical_uv.ply'])

            if save_gaussian:
                print("  ğŸ’¾ Gaussian PLYï¼ˆå…¬å¼ï¼‰ã‚’ä¿å­˜ä¸­...")
                ubody_gaussians.save_gaussian_ply(video_output_dir, save_split=save_split)
                
                # å…¬å¼PLYãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
                official_ply = os.path.join(video_output_dir, 'GS_canonical.ply')
                
                # ä¸‰è§’å½¢ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ã—ãŸæ–°ã—ã„PLYãƒ•ã‚¡ã‚¤ãƒ«
                enhanced_ply = os.path.join(video_output_dir, 'GS_canonical_full.ply')
                
                # EHMãƒ¡ãƒƒã‚·ãƒ¥ã‹ã‚‰ä¸‰è§’å½¢ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                print("  ğŸ”§ ä¸‰è§’å½¢ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ä¸­...")
                try:
                    faces = ubody_gaussians.smplx.faces_tensor.cpu().numpy()
                    num_faces = faces.shape[0]
                    print(f"    ğŸ“ EHMãƒ¡ãƒƒã‚·ãƒ¥: {num_faces:,} ä¸‰è§’å½¢")
                    
                    # ä¸‰è§’å½¢ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
                    add_faces_to_ply(official_ply, enhanced_ply, faces)
                    
                    print(f"    âœ… ä¸‰è§’å½¢ãƒ‡ãƒ¼ã‚¿è¿½åŠ å®Œäº†")
                    ply_files.append('GS_canonical_full.ply')
                    
                    # æ¤œè¨¼
                    if verify_format:
                        print(f"\n  ğŸ” PLYãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼: GS_canonical_full.ply")
                        verification = verify_ply_format(enhanced_ply)
                        
                        if "error" in verification:
                            print(f"    âŒ ã‚¨ãƒ©ãƒ¼: {verification['error']}")
                        else:
                            print(f"    âœ… é ‚ç‚¹æ•°: {verification['vertex_count']:,}")
                            print(f"    âœ… ä¸‰è§’å½¢æ•°: {verification['face_count']:,}")
                            print(f"    âœ… ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£æ•°: {verification['property_count']}")
                            print(f"    ğŸ“Š ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {verification['file_size_mb']:.2f} MB")
                            print(f"    ğŸ“ ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºä¸€è‡´: {'âœ…' if verification['size_match'] else 'âŒ'}")
                            print(f"    ğŸ”¹ ä¸‰è§’å½¢ãƒ‡ãƒ¼ã‚¿: {'âœ… ã‚ã‚Š' if verification['has_faces'] else 'âŒ ãªã—'}")
                            print(f"    ğŸ”¹ Opacityå±æ€§: {'âœ… ã‚ã‚Š' if verification['has_opacity'] else 'âŒ ãªã—'}")
                            print(f"    ğŸ”¹ Rotationå±æ€§: {'âœ… ã‚ã‚Š' if verification['has_rotation'] else 'âŒ ãªã—'}")
                            print(f"    ğŸ”¹ Scaleå±æ€§: {'âœ… ã‚ã‚Š' if verification['has_scale'] else 'âŒ ãªã—'}")
                        
                        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
                        if extract_samples:
                            print(f"\n  ğŸ“Š ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿æŠ½å‡º:")
                            extract_ply_sample(enhanced_ply, num_samples=3)
                    
                except Exception as e:
                    print(f"    âŒ ä¸‰è§’å½¢ãƒ‡ãƒ¼ã‚¿è¿½åŠ ã‚¨ãƒ©ãƒ¼: {e}")
                    import traceback
                    traceback.print_exc()

            # gvrm.ts/ply.tsäº’æ›ã®Web PLYã‚’ç”Ÿæˆ
            if save_web:
                print("  ğŸ’¾ Web PLY (gvrm.tsäº’æ›)ã‚’ä¿å­˜ä¸­...")
                try:
                    faces = ubody_gaussians.smplx.faces_tensor.cpu().numpy()
                    web_ply_path, web_num_points = save_web_compatible_ply(
                        ubody_gaussians, video_output_dir, faces=faces
                    )
                    ply_files.append('avatar_web.ply')
                    print(f"    âœ… avatar_web.ply ä¿å­˜å®Œäº† ({web_num_points:,} é ‚ç‚¹)")

                    # Web PLYã®æ¤œè¨¼
                    if verify_format:
                        print(f"\n  ğŸ” PLYãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼: avatar_web.ply")
                        web_verification = verify_ply_format(web_ply_path)
                        if "error" not in web_verification:
                            print(f"    âœ… é ‚ç‚¹æ•°: {web_verification['vertex_count']:,}")
                            print(f"    âœ… ä¸‰è§’å½¢æ•°: {web_verification['face_count']:,}")
                            print(f"    âœ… ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£æ•°: {web_verification['property_count']} (æœŸå¾…: 12)")
                            print(f"    ğŸ“Š ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {web_verification['file_size_mb']:.2f} MB")
                except Exception as e:
                    print(f"    âŒ Web PLYä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
                    import traceback
                    traceback.print_exc()

            # çµ±è¨ˆæƒ…å ±
            num_template = vertex_gs_dict['positions'].shape[1]
            num_uv = up_point_gs_dict['opacities'].shape[1]
            total = num_template + num_uv

            result = {
                'video_id': video_id,
                'output_dir': video_output_dir,
                'ply_files': ply_files,
                'num_template_gaussians': int(num_template),
                'num_uv_gaussians': int(num_uv),
                'total_gaussians': int(total),
                'inference_time_ms': infer_time * 1000
            }
            
            # æ¤œè¨¼çµæœã‚’è¿½åŠ 
            if verify_format and save_gaussian and os.path.exists(enhanced_ply):
                verification = verify_ply_format(enhanced_ply)
                if "error" not in verification:
                    result['verification'] = verification
            
            results.append(result)

            print(f"\n  âœ… å®Œäº†: Template={num_template:,}, UV={num_uv:,}, Total={total:,} ({infer_time*1000:.1f}ms)")

    # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    test_dataset._lmdb_engine.close()

    # ã‚µãƒãƒªãƒ¼ä¿å­˜
    summary = {
        'data_path': data_path,
        'output_dir': output_dir,
        'total_videos': len(results),
        'format': 'official_gaussian_ply_with_faces',
        'paper_compliant': True,
        'has_triangles': True,
        'settings': {
            'save_split': save_split,
            'save_point_cloud': save_point_cloud,
            'save_gaussian': save_gaussian,
            'save_web': save_web,
            'verify_format': verify_format
        },
        'videos': results
    }

    summary_path = os.path.join(output_dir, 'generation_summary.json')
    with open(summary_path, 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)

    print("\n" + "=" * 70)
    print("ğŸ‰ ç”Ÿæˆå®Œäº†!")
    print("=" * 70)
    print(f"ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {output_dir}")
    print(f"ğŸ“Š å‡¦ç†å‹•ç”»æ•°: {len(results)}")
    print(f"ğŸ“ ã‚µãƒãƒªãƒ¼: {summary_path}")
    
    # æ¤œè¨¼ã‚µãƒãƒªãƒ¼
    if verify_format and results:
        print("\nğŸ“‹ æ¤œè¨¼ã‚µãƒãƒªãƒ¼:")
        for result in results:
            if 'verification' in result:
                v = result['verification']
                print(f"  {result['video_id']}:")
                print(f"    - é ‚ç‚¹: {v['vertex_count']:,}, ä¸‰è§’å½¢: {v['face_count']:,}")
                print(f"    - ä¸‰è§’å½¢ãƒ‡ãƒ¼ã‚¿: {'âœ…' if v['has_faces'] else 'âŒ'}")
                print(f"    - å¿…é ˆå±æ€§: Opacity={'âœ…' if v['has_opacity'] else 'âŒ'}, "
                      f"Rotation={'âœ…' if v['has_rotation'] else 'âŒ'}, "
                      f"Scale={'âœ…' if v['has_scale'] else 'âŒ'}")

    # Volume ã‚³ãƒŸãƒƒãƒˆ
    ply_output_volume.commit()
    print("\nâœ… PLY files committed to Volume")

    return summary


@app.local_entrypoint()
def main(
    output_name: str = "guava_avatar",
    split: bool = False,
    point_cloud: bool = False,
    gaussian: bool = True,
    web: bool = True,
    web_only: bool = False,
    no_verify: bool = False,
    no_samples: bool = False
):
    """
    PLYç”Ÿæˆã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ

    ä½¿ç”¨ä¾‹:
        modal run generate_ply_cloud.py
        modal run generate_ply_cloud.py --output-name my_avatar
        modal run generate_ply_cloud.py --split
        modal run generate_ply_cloud.py --web-only  # avatar_web.plyã®ã¿ç”Ÿæˆ
        modal run generate_ply_cloud.py --no-verify
    """
    print("=" * 70)
    print("ğŸš€ GUAVA PLY Generator (ä¸‰è§’å½¢ãƒ‡ãƒ¼ã‚¿è¿½åŠ ç‰ˆ)")
    print("ğŸ“– å…¬å¼PLY + EHMãƒ¡ãƒƒã‚·ãƒ¥ã®ä¸‰è§’å½¢ãƒ‡ãƒ¼ã‚¿ + gvrm.tsäº’æ›Web PLY")
    print("=" * 70)

    # web_onlyã®å ´åˆã¯ä»–ã®PLYã‚’ç„¡åŠ¹åŒ–
    if web_only:
        gaussian = False
        point_cloud = False
        split = False
        web = True

    result = generate_ply.remote(
        output_name=output_name,
        save_split=split,
        save_point_cloud=point_cloud,
        save_gaussian=gaussian,
        save_web=web,
        verify_format=not no_verify,
        extract_samples=not no_samples
    )

    if result:
        print("\n" + "=" * 70)
        print("ğŸ“Š ç”Ÿæˆçµæœã‚µãƒãƒªãƒ¼")
        print("=" * 70)
        print(f"ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {result['output_dir']}")
        print(f"ğŸ“Š å‡¦ç†å‹•ç”»æ•°: {result['total_videos']}")
        print(f"ğŸ“– ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: {result['format']}")
        print(f"âœ… è«–æ–‡æº–æ‹ : {result['paper_compliant']}")
        print(f"âœ… ä¸‰è§’å½¢ãƒ‡ãƒ¼ã‚¿: {result['has_triangles']}")
        for video in result['videos']:
            print(f"\n  ğŸ“¹ {video['video_id']}:")
            print(f"    ğŸ”¹ Template Gaussians: {video['num_template_gaussians']:,}")
            print(f"    ğŸ”¹ UV Gaussians: {video['num_uv_gaussians']:,}")
            print(f"    ğŸ”¹ Total: {video['total_gaussians']:,}")
            print(f"    âš¡ æ¨è«–æ™‚é–“: {video['inference_time_ms']:.1f}ms")
            print(f"    ğŸ“„ PLYãƒ•ã‚¡ã‚¤ãƒ«:")
            for ply in video['ply_files']:
                print(f"      - {ply}")
            
            if 'verification' in video:
                v = video['verification']
                print(f"    âœ… æ¤œè¨¼:")
                print(f"      - é ‚ç‚¹æ•°: {v['vertex_count']:,}")
                print(f"      - ä¸‰è§’å½¢æ•°: {v['face_count']:,}")
                print(f"      - ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {v['file_size_mb']:.2f} MB")

        print("\n" + "=" * 70)
        print("âœ… PLYç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸ")
        print("=" * 70)
    else:
        print("\n" + "=" * 70)
        print("âŒ PLYç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
        print("=" * 70)