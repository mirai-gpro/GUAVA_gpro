import onnxruntime as ort
import numpy as np
import os

def check_model():
    model_path = "template_decoder.onnx" # ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§ãƒã‚§ãƒƒã‚¯
    
    print(f"ğŸ” Checking model: {model_path}")
    if not os.path.exists(model_path):
        print("âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return

    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºç¢ºèª
    size_mb = os.path.getsize(model_path) / (1024*1024)
    print(f"ğŸ“¦ File Size: {size_mb:.2f} MB")
    
    if size_mb < 0.1:
        print("âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒå°ã•ã™ãã¾ã™ï¼é‡ã¿ãŒå…¥ã£ã¦ã„ã¾ã›ã‚“ã€‚")
        return

    try:
        # ONNX Runtimeã§ãƒ­ãƒ¼ãƒ‰
        session = ort.InferenceSession(model_path)
        print("âœ… Model loaded successfully by ONNX Runtime.")
    except Exception as e:
        print(f"âŒ Model load failed: {e}")
        return

    # ãƒ€ãƒŸãƒ¼å…¥åŠ›ä½œæˆ
    # TypeScriptå´ã¨åŒã˜æ¡ä»¶: N=100
    num_vertices = 100
    
    # fused_features: [1, N, 512]
    # stats from logs: min=-5, max=5 (approx)
    fused = np.random.uniform(-5, 5, (1, num_vertices, 512)).astype(np.float32)
    
    # view_dirs: [1, 27]
    dirs = np.zeros((1, 27), dtype=np.float32)
    dirs[0, 2] = 1.0 # Z-axis

    # æ¨è«–å®Ÿè¡Œ
    print("ğŸƒ Running inference check...")
    try:
        inputs = {
            'fused_features': fused,
            'view_dirs': dirs
        }
        outputs = session.run(None, inputs)
        
        # å‡ºåŠ›ãƒã‚§ãƒƒã‚¯
        output_names = [o.name for o in session.get_outputs()]
        has_nan = False
        
        for name, val in zip(output_names, outputs):
            if np.isnan(val).any() or np.isinf(val).any():
                print(f"âŒ Output '{name}' contains NaN or Inf!")
                has_nan = True
                # çµ±è¨ˆã‚’è¡¨ç¤º
                print(f"   Min: {np.min(val)}, Max: {np.max(val)}")
            else:
                print(f"âœ… Output '{name}': OK (Mean: {np.mean(val):.4f})")
        
        if not has_nan:
            print("\nğŸ‰ MODEL IS HEALTHY!")
            print("Pythonä¸Šã§ã¯æ­£å¸¸ã«å‹•ä½œã—ã¦ã„ã¾ã™ã€‚")
            print("â†’ ãƒ–ãƒ©ã‚¦ã‚¶ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒåŸå› ã®å¯èƒ½æ€§ãŒé«˜ã„ã§ã™ã€‚")
            print("â†’ ãƒ–ãƒ©ã‚¦ã‚¶ã§ã€ã‚¹ãƒ¼ãƒ‘ãƒ¼ãƒªãƒ­ãƒ¼ãƒ‰ (Ctrl+F5 / Cmd+Shift+R)ã€ã‚’è©¦ã—ã¦ãã ã•ã„ã€‚")
        else:
            print("\nğŸ’€ MODEL IS BROKEN.")
            print("Pythonä¸Šã§ã‚‚NaNãŒå‡ºã¾ã™ã€‚é‡ã¿ã®ãƒ­ãƒ¼ãƒ‰æ‰‹é †ã«å•é¡ŒãŒã‚ã‚Šã¾ã™ã€‚")

    except Exception as e:
        print(f"âŒ Inference failed: {e}")

if __name__ == "__main__":
    check_model()