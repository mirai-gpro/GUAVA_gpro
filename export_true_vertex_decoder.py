import torch
import torch.nn as nn
import onnx
import os
import numpy as np

# ==============================================================================
# 1. ã‚¯ãƒ©ã‚¹å®šç¾© (Offsetå±¤ã‚’å‰Šé™¤ã—ãŸä¿®æ­£ç‰ˆ)
# ==============================================================================
class Vertex_GS_Decoder_Fixed(nn.Module):
    def __init__(self, in_dim=512, dir_dim=27, color_out_dim=32):
        super().__init__()
        
        # ç‰¹å¾´é‡æŠ½å‡º (MLP)
        self.feature_layers = nn.Sequential(
            nn.Linear(in_dim, in_dim//2, bias=True),
            nn.ReLU(inplace=True),
            nn.Linear(in_dim//2, in_dim//2, bias=True),
            nn.ReLU(inplace=True),
            nn.Linear(in_dim//2, in_dim//2, bias=True),
            nn.ReLU(inplace=True),
            nn.Linear(in_dim//2, in_dim//2, bias=True),
        )
        
        layer_in_dim = in_dim//2 + dir_dim
        
        # å„å±æ€§ã®ãƒ˜ãƒƒãƒ‰
        self.color_layers = nn.Sequential(
            nn.Linear(layer_in_dim, 128, bias=True),
            nn.ReLU(inplace=True),
            nn.Linear(128, color_out_dim, bias=True),
        )
        self.opacity_layers = nn.Sequential(
            nn.Linear(layer_in_dim, 128, bias=True),
            nn.ReLU(inplace=True),
            nn.Linear(128, 1, bias=True),
        )
        self.scale_layers = nn.Sequential(
            nn.Linear(layer_in_dim, 128, bias=True),
            nn.ReLU(inplace=True),
            nn.Linear(128, 3, bias=True),
        )
        self.rotation_layers = nn.Sequential(
            nn.Linear(layer_in_dim, 128, bias=True),
            nn.ReLU(inplace=True),
            nn.Linear(128, 4, bias=True),
        )
        
        # â˜…å‰Šé™¤: offset_layers ã¯é‡ã¿ãƒ•ã‚¡ã‚¤ãƒ«ã«å­˜åœ¨ã—ãªã„ãŸã‚å‰Šé™¤
        # self.offset_layers = ... 

    def forward(self, fused_features, view_dirs):
        # 1. ç‰¹å¾´é‡å¤‰æ›
        features = self.feature_layers(fused_features)
        
        # 2. ViewDirã®æ‹¡å¼µã¨çµåˆ
        B, N, _ = features.shape
        dirs_expanded = view_dirs.unsqueeze(1).expand(B, N, -1)
        features_cat = torch.cat([features, dirs_expanded], dim=-1)
        
        # 3. å„å±æ€§ã®äºˆæ¸¬
        rgb = self.color_layers(features_cat)
        
        # å®‰å…¨ãªæ´»æ€§åŒ–é–¢æ•°
        opacity = torch.sigmoid(self.opacity_layers(features_cat))
        scale = torch.exp(self.scale_layers(features_cat))
        rotation = torch.nn.functional.normalize(self.rotation_layers(features_cat), dim=-1)
        
        # â˜…ä¿®æ­£: Offsetã¯ã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«ã‚’å‡ºåŠ›
        # ãƒ¢ãƒ‡ãƒ«ãŒå­¦ç¿’ã—ã¦ã„ãªã„ãŸã‚ã€å¤‰ã«äºˆæ¸¬ã•ã›ãšã€Œå‹•ã‹ãªã„ã€ã¨ã™ã‚‹ã®ãŒæ­£è§£
        offset = torch.zeros(B, N, 3, device=features.device)
        
        return rgb, opacity, scale, rotation, offset

# ==============================================================================
# 2. ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Ÿè¡Œ
# ==============================================================================
def export_pipeline():
    print("ğŸš€ Starting Corrected Export Pipeline (No Offset)...")
    
    weights_path = "best_160000.pt"
    if not os.path.exists(weights_path):
        print(f"âŒ '{weights_path}' ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    checkpoint = torch.load(weights_path, map_location="cpu")
    state_dict = checkpoint.get('state_dict', checkpoint.get('model', checkpoint))

    # Base Features ã®å†ä¿å­˜ (å¿µã®ãŸã‚)
    if 'vertex_base_feature' in state_dict:
        base_feat = state_dict['vertex_base_feature'].float().numpy()
        base_feat.tofile("base_features.bin")
        print(f"âœ… Base Features saved ({base_feat.shape})")

    # Decoderé‡ã¿ã®æŠ½å‡º
    decoder_dict = {}
    prefix = "vertex_gs_decoder."
    for k, v in state_dict.items():
        if k.startswith(prefix):
            decoder_dict[k[len(prefix):]] = v

    # ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
    model = Vertex_GS_Decoder_Fixed(in_dim=512, dir_dim=27, color_out_dim=32)
    
    # ãƒ­ãƒ¼ãƒ‰ (ä»Šåº¦ã¯Strict=Trueã§é€šã‚‹ã¯ãšï¼)
    try:
        model.load_state_dict(decoder_dict, strict=True)
        print("ğŸ‰ Weights loaded PERFECTLY (Strict Match)!")
    except Exception as e:
        print(f"âš ï¸ Load warning: {e}")
        # OffsetãŒãªã„ã®ã§ã‚¨ãƒ©ãƒ¼ã¯å‡ºãªã„ã¯ãšã ãŒã€å¿µã®ãŸã‚
        model.load_state_dict(decoder_dict, strict=False)

    model.eval()

    # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
    print("ğŸ“¦ Exporting corrected ONNX...")
    N = 100
    dummy_feat = torch.randn(1, N, 512)
    dummy_dir = torch.randn(1, 27)

    output_file = "template_decoder.onnx"

    torch.onnx.export(
        model,
        (dummy_feat, dummy_dir),
        output_file,
        input_names=['fused_features', 'view_dirs'],
        output_names=['rgb', 'opacity', 'scale', 'rotation', 'offset'],
        opset_version=14,
        dynamic_axes={
            'fused_features': {1: 'num_vertices'},
            'rgb': {1: 'num_vertices'},
            'opacity': {1: 'num_vertices'},
            'scale': {1: 'num_vertices'},
            'rotation': {1: 'num_vertices'},
            'offset': {1: 'num_vertices'}
        }
    )
    
    print(f"âœ… Export Complete: {output_file}")
    print("ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ assets ã«ä¸Šæ›¸ãã—ã¦ãã ã•ã„ã€‚ã“ã‚Œã§NaNåœ°ç„ã‹ã‚‰è§£æ”¾ã•ã‚Œã¾ã™ã€‚")

if __name__ == "__main__":
    export_pipeline()