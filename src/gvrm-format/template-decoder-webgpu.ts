/**
 * Template Decoder - WebGPU Compute Shader Implementation
 * 
 * ONNX„ÇíÂÆåÂÖ®„Å´„Éê„Ç§„Éë„Çπ„Åó„Å¶„ÄÅWebGPU Compute Shader„ÅßÁõ¥Êé•ÂÆüË°å
 * ONNX Runtime Web„ÅÆÂãïÁöÑÂΩ¢Áä∂ÂïèÈ°å„ÇíÂÆåÂÖ®ÂõûÈÅø
 * 
 * „Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£:
 * 1. Global Feature Mapping: 768‚Üí256 (3Â±§FC)
 * 2. Feature Concatenation: projection[128] + base[128] + global[256] = 512
 * 3. GS Decoder Feature Layers: 512‚Üí256‚Üí256‚Üí256‚Üí256 (4Â±§)
 * 4. Attribute Heads: 256+27=283 ‚Üí rgb[32], opacity[1], scale[3], rotation[4]
 */

export interface TemplateDecoderInput {
  projection_features: Float32Array;   // [N, 128]
  global_embedding: Float32Array;      // [768]
  num_vertices: number;
}

export interface TemplateDecoderOutput {
  positions: Float32Array;   // [N, 3]
  rotations: Float32Array;   // [N, 4]
  scales: Float32Array;      // [N, 3]
  opacities: Float32Array;   // [N, 1]
  colors: Float32Array;      // [N, 32]
  id_embedding: Float32Array; // [256]
}

interface ModelWeights {
  // Global Feature Mapping (768‚Üí256‚Üí256‚Üí256)
  global_fc0_weight: Float32Array;  // [256, 768]
  global_fc0_bias: Float32Array;    // [256]
  global_fc2_weight: Float32Array;  // [256, 256]
  global_fc2_bias: Float32Array;    // [256]
  global_fc4_weight: Float32Array;  // [256, 256]
  global_fc4_bias: Float32Array;    // [256]
  
  // Base Features
  base_features: Float32Array;      // [10595, 128]
  
  // GS Decoder Feature Layers (512‚Üí256‚Üí256‚Üí256‚Üí256)
  feature_0_weight: Float32Array;   // [256, 512]
  feature_0_bias: Float32Array;     // [256]
  feature_2_weight: Float32Array;   // [256, 256]
  feature_2_bias: Float32Array;     // [256]
  feature_4_weight: Float32Array;   // [256, 256]
  feature_4_bias: Float32Array;     // [256]
  feature_6_weight: Float32Array;   // [256, 256]
  feature_6_bias: Float32Array;     // [256]
  
  // Attribute Heads (283‚Üí128‚Üíoutput)
  color_0_weight: Float32Array;     // [128, 283]
  color_0_bias: Float32Array;       // [128]
  color_2_weight: Float32Array;     // [32, 128]
  color_2_bias: Float32Array;       // [32]
  
  opacity_0_weight: Float32Array;   // [128, 283]
  opacity_0_bias: Float32Array;     // [128]
  opacity_2_weight: Float32Array;   // [1, 128]
  opacity_2_bias: Float32Array;     // [1]
  
  scale_0_weight: Float32Array;     // [128, 283]
  scale_0_bias: Float32Array;       // [128]
  scale_2_weight: Float32Array;     // [3, 128]
  scale_2_bias: Float32Array;       // [3]
  
  rotation_0_weight: Float32Array;  // [128, 283]
  rotation_0_bias: Float32Array;    // [128]
  rotation_2_weight: Float32Array;  // [4, 128]
  rotation_2_bias: Float32Array;    // [4]
}

/**
 * Template Decoder using WebGPU Compute Shaders
 * 
 * ONNX„ÇíÂÆåÂÖ®„Å´„Éê„Ç§„Éë„Çπ„Åó„Å¶„ÄÅGPU‰∏ä„ÅßÁõ¥Êé•Êé®Ë´ñ„ÇíÂÆüË°å
 */
export class TemplateDecoderWebGPU {
  private device: GPUDevice | null = null;
  private initialized: boolean = false;
  private weights: ModelWeights | null = null;
  private callCount: number = 0;
  
  // GPU Buffers for weights
  private weightBuffers: Map<string, GPUBuffer> = new Map();
  
  // Compute pipelines
  private globalMappingPipeline: GPUComputePipeline | null = null;
  private featureLayersPipeline: GPUComputePipeline | null = null;
  private attributeHeadsPipeline: GPUComputePipeline | null = null;

  /**
   * Initialize Template Decoder with WebGPU
   */
  async init(device: GPUDevice, assetsPath: string = '/assets'): Promise<void> {
    console.log('[TemplateDecoderWebGPU] Initializing...');
    
    this.device = device;
    
    try {
      // Load weights
      await this.loadWeights(assetsPath);
      
      // Create compute pipelines
      await this.createPipelines();
      
      this.initialized = true;
      console.log('[TemplateDecoderWebGPU] ‚úÖ Initialization complete');
      
    } catch (error) {
      console.error('[TemplateDecoderWebGPU] ‚ùå Initialization failed:', error);
      throw error;
    }
  }

  /**
   * Load model weights from binary files
   */
  private async loadWeights(assetsPath: string): Promise<void> {
    console.log('[TemplateDecoderWebGPU]   Loading weights...');
    
    // Load base_features.bin
    const baseResponse = await fetch(`${assetsPath}/base_features.bin`);
    const baseBuffer = await baseResponse.arrayBuffer();
    const baseFeatures = new Float32Array(baseBuffer);
    console.log(`[TemplateDecoderWebGPU]   ‚úÖ Base features: ${baseFeatures.length / 128} vertices`);
    
    // Load weights.bin (all FC layer weights)
    const weightsResponse = await fetch(`${assetsPath}/template_decoder_weights.bin`);
    const weightsBuffer = await weightsResponse.arrayBuffer();
    const allWeights = new Float32Array(weightsBuffer);
    console.log(`[TemplateDecoderWebGPU]   ‚úÖ Weights loaded: ${allWeights.length} floats`);
    
    // Parse weights (È†ÜÂ∫è„ÅØ export script „ÅßÂÆöÁæ©)
    let offset = 0;
    
    const readArray = (size: number): Float32Array => {
      const arr = allWeights.slice(offset, offset + size);
      offset += size;
      return arr;
    };
    
    this.weights = {
      // Global Feature Mapping
      global_fc0_weight: readArray(256 * 768),
      global_fc0_bias: readArray(256),
      global_fc2_weight: readArray(256 * 256),
      global_fc2_bias: readArray(256),
      global_fc4_weight: readArray(256 * 256),
      global_fc4_bias: readArray(256),
      
      // Base features (loaded separately)
      base_features: baseFeatures,
      
      // Feature layers
      feature_0_weight: readArray(256 * 512),
      feature_0_bias: readArray(256),
      feature_2_weight: readArray(256 * 256),
      feature_2_bias: readArray(256),
      feature_4_weight: readArray(256 * 256),
      feature_4_bias: readArray(256),
      feature_6_weight: readArray(256 * 256),
      feature_6_bias: readArray(256),
      
      // Color head
      color_0_weight: readArray(128 * 283),
      color_0_bias: readArray(128),
      color_2_weight: readArray(32 * 128),
      color_2_bias: readArray(32),
      
      // Opacity head
      opacity_0_weight: readArray(128 * 283),
      opacity_0_bias: readArray(128),
      opacity_2_weight: readArray(1 * 128),
      opacity_2_bias: readArray(1),
      
      // Scale head
      scale_0_weight: readArray(128 * 283),
      scale_0_bias: readArray(128),
      scale_2_weight: readArray(3 * 128),
      scale_2_bias: readArray(3),
      
      // Rotation head
      rotation_0_weight: readArray(128 * 283),
      rotation_0_bias: readArray(128),
      rotation_2_weight: readArray(4 * 128),
      rotation_2_bias: readArray(4),
    };
    
    console.log(`[TemplateDecoderWebGPU]   ‚úÖ Weights parsed (${offset} floats used)`);
  }

  /**
   * Create WebGPU compute pipelines
   */
  private async createPipelines(): Promise<void> {
    if (!this.device) throw new Error('Device not initialized');
    
    console.log('[TemplateDecoderWebGPU]   Creating compute pipelines...');
    
    // For now, we'll use CPU fallback since compute shaders are complex
    // This can be upgraded to full GPU compute later
    
    console.log('[TemplateDecoderWebGPU]   ‚úÖ Pipelines created (CPU fallback mode)');
  }

  /**
   * Run Template Decoder inference (CPU implementation)
   * 
   * This is a CPU fallback that guarantees correctness.
   * Can be upgraded to WebGPU Compute Shaders for better performance.
   */
  async forward(input: TemplateDecoderInput): Promise<TemplateDecoderOutput> {
    if (!this.initialized || !this.weights) {
      throw new Error('Template Decoder not initialized');
    }

    const N = input.num_vertices;
    const weights = this.weights;
    
    this.callCount++;
    console.log(`[TemplateDecoderWebGPU] ========== Call #${this.callCount} ==========`);
    console.log(`[TemplateDecoderWebGPU]   Vertices: ${N}`);

    // ================================================================
    // Step 1: Global Feature Mapping (768 ‚Üí 256)
    // PythonÁâà ubody_gaussian.py Ê∫ñÊã†: LeakyReLU‰ΩøÁî®
    // ================================================================
    const global_256 = this.linearLeakyRelu(
      input.global_embedding,
      weights.global_fc0_weight, weights.global_fc0_bias,
      768, 256
    );
    const global_256_2 = this.linearLeakyRelu(
      global_256,
      weights.global_fc2_weight, weights.global_fc2_bias,
      256, 256
    );
    const id_embedding = this.linear(
      global_256_2,
      weights.global_fc4_weight, weights.global_fc4_bias,
      256, 256
    );
    
    console.log(`[TemplateDecoderWebGPU]   Global mapping: 768 ‚Üí 256 ‚úÖ`);

    // ================================================================
    // Step 2: Get base features for N vertices
    // ================================================================
    const base_features = weights.base_features.slice(0, N * 128);
    console.log(`[TemplateDecoderWebGPU]   Base features: ${N} x 128 ‚úÖ`);

    // ================================================================
    // Step 3: Concatenate features for each vertex
    // fused = [projection[128], base[128], global[256]] = [512]
    // ================================================================
    const fused = new Float32Array(N * 512);
    for (let i = 0; i < N; i++) {
      const offset = i * 512;
      // projection [128]
      for (let j = 0; j < 128; j++) {
        fused[offset + j] = input.projection_features[i * 128 + j];
      }
      // base [128]
      for (let j = 0; j < 128; j++) {
        fused[offset + 128 + j] = base_features[i * 128 + j];
      }
      // global [256] (broadcast)
      for (let j = 0; j < 256; j++) {
        fused[offset + 256 + j] = id_embedding[j];
      }
    }
    console.log(`[TemplateDecoderWebGPU]   Fused features: ${N} x 512 ‚úÖ`);

    // ================================================================
    // Step 4: Feature layers (512‚Üí256‚Üí256‚Üí256‚Üí256)
    // PythonÁâà feature_decoder.py Vertex_GS_Decoder Ê∫ñÊã†:
    //   - ÊúÄÂàù„ÅÆ3Â±§: Linear + ReLU
    //   - ÊúÄÂæå„ÅÆÂ±§: Linear „ÅÆ„Åø (ReLUÁÑ°„Åó)
    // ================================================================
    let features = this.batchLinearRelu(fused, weights.feature_0_weight, weights.feature_0_bias, N, 512, 256);
    features = this.batchLinearRelu(features, weights.feature_2_weight, weights.feature_2_bias, N, 256, 256);
    features = this.batchLinearRelu(features, weights.feature_4_weight, weights.feature_4_bias, N, 256, 256);
    features = this.batchLinear(features, weights.feature_6_weight, weights.feature_6_bias, N, 256, 256);  // NO ReLU
    console.log(`[TemplateDecoderWebGPU]   Feature layers: ${N} x 256 ‚úÖ`);

    // ================================================================
    // Step 5: Concatenate with view_dirs (256 + 27 = 283)
    // view_dirs is all zeros for now
    // ================================================================
    const features_with_view = new Float32Array(N * 283);
    for (let i = 0; i < N; i++) {
      const srcOffset = i * 256;
      const dstOffset = i * 283;
      for (let j = 0; j < 256; j++) {
        features_with_view[dstOffset + j] = features[srcOffset + j];
      }
      // view_dirs [27] = 0
      for (let j = 0; j < 27; j++) {
        features_with_view[dstOffset + 256 + j] = 0;
      }
    }

    // ================================================================
    // Step 6: Attribute heads
    // ================================================================
    // RGB: 283‚Üí128‚Üí32
    let rgb_hidden = this.batchLinearRelu(features_with_view, weights.color_0_weight, weights.color_0_bias, N, 283, 128);
    const colors = this.batchLinear(rgb_hidden, weights.color_2_weight, weights.color_2_bias, N, 128, 32);
    
    // Opacity: 283‚Üí128‚Üí1 + sigmoid
    let opacity_hidden = this.batchLinearRelu(features_with_view, weights.opacity_0_weight, weights.opacity_0_bias, N, 283, 128);
    const opacities_raw = this.batchLinear(opacity_hidden, weights.opacity_2_weight, weights.opacity_2_bias, N, 128, 1);
    const opacities = new Float32Array(N);
    for (let i = 0; i < N; i++) {
      opacities[i] = 1 / (1 + Math.exp(-opacities_raw[i])); // sigmoid
    }
    
    // Scale: 283‚Üí128‚Üí3 + sigmoid * 0.05
    // PythonÁâà Vertex_GS_DecoderÊ∫ñÊã†: scales = torch.sigmoid(scales) * 0.05
    // Âá∫ÂäõÁØÑÂõ≤: [0, 0.05]
    let scale_hidden = this.batchLinearRelu(features_with_view, weights.scale_0_weight, weights.scale_0_bias, N, 283, 128);
    const scales_raw = this.batchLinear(scale_hidden, weights.scale_2_weight, weights.scale_2_bias, N, 128, 3);
    const scales = new Float32Array(N * 3);
    for (let i = 0; i < N * 3; i++) {
      const sigmoid = 1 / (1 + Math.exp(-scales_raw[i]));
      scales[i] = sigmoid * 0.05;
    }
    
    // Rotation: 283‚Üí128‚Üí4 + normalize
    let rotation_hidden = this.batchLinearRelu(features_with_view, weights.rotation_0_weight, weights.rotation_0_bias, N, 283, 128);
    const rotations_raw = this.batchLinear(rotation_hidden, weights.rotation_2_weight, weights.rotation_2_bias, N, 128, 4);
    const rotations = new Float32Array(N * 4);
    for (let i = 0; i < N; i++) {
      const offset = i * 4;
      let norm = 0;
      for (let j = 0; j < 4; j++) {
        norm += rotations_raw[offset + j] * rotations_raw[offset + j];
      }
      norm = Math.sqrt(norm) + 1e-8;
      for (let j = 0; j < 4; j++) {
        rotations[offset + j] = rotations_raw[offset + j] / norm;
      }
    }

    console.log(`[TemplateDecoderWebGPU]   Attribute heads ‚úÖ`);

    // ================================================================
    // Output statistics
    // ================================================================
    const opacityStats = this.analyzeArray(opacities);
    const scaleStats = this.analyzeArray(scales);
    const rotationStats = this.analyzeArray(rotations);
    const colorStats = this.analyzeArray(colors);
    
    console.log(`[TemplateDecoderWebGPU] üì§ Output Stats:`);
    console.log(`[TemplateDecoderWebGPU]   Opacity:  min=${opacityStats.min.toFixed(6)}, max=${opacityStats.max.toFixed(6)}, unique=${opacityStats.unique}`);
    console.log(`[TemplateDecoderWebGPU]   Scale:    min=${scaleStats.min.toFixed(6)}, max=${scaleStats.max.toFixed(6)}, unique=${scaleStats.unique}`);
    console.log(`[TemplateDecoderWebGPU]   Rotation: min=${rotationStats.min.toFixed(6)}, max=${rotationStats.max.toFixed(6)}, unique=${rotationStats.unique}`);
    console.log(`[TemplateDecoderWebGPU]   RGB:      min=${colorStats.min.toFixed(6)}, max=${colorStats.max.toFixed(6)}, unique=${colorStats.unique}`);
    
    console.log(`[TemplateDecoderWebGPU] üìù Opacity Sample (first 10): [${Array.from(opacities.slice(0, 10)).map(v => v.toFixed(6)).join(', ')}]`);

    // Positions are not modified by decoder
    const positions = new Float32Array(N * 3);

    return {
      positions,
      rotations,
      scales,
      opacities,
      colors,
      id_embedding
    };
  }

  // ================================================================
  // Helper functions for linear algebra
  // ================================================================

  /**
   * Linear layer: output = input @ weight.T + bias
   */
  private linear(input: Float32Array, weight: Float32Array, bias: Float32Array, inDim: number, outDim: number): Float32Array {
    const output = new Float32Array(outDim);
    for (let o = 0; o < outDim; o++) {
      let sum = bias[o];
      for (let i = 0; i < inDim; i++) {
        sum += input[i] * weight[o * inDim + i];
      }
      output[o] = sum;
    }
    return output;
  }

  /**
   * Linear + ReLU
   */
  private linearRelu(input: Float32Array, weight: Float32Array, bias: Float32Array, inDim: number, outDim: number): Float32Array {
    const output = this.linear(input, weight, bias, inDim, outDim);
    for (let i = 0; i < output.length; i++) {
      output[i] = Math.max(0, output[i]);
    }
    return output;
  }

  /**
   * Linear + LeakyReLU (negative_slope=0.01, PyTorch default)
   * PythonÁâà ubody_gaussian.py „ÅÆ global_feature_mapping Ê∫ñÊã†
   */
  private linearLeakyRelu(input: Float32Array, weight: Float32Array, bias: Float32Array, inDim: number, outDim: number): Float32Array {
    const output = this.linear(input, weight, bias, inDim, outDim);
    for (let i = 0; i < output.length; i++) {
      output[i] = output[i] > 0 ? output[i] : 0.01 * output[i];
    }
    return output;
  }

  /**
   * Batch linear: process N samples
   */
  private batchLinear(input: Float32Array, weight: Float32Array, bias: Float32Array, N: number, inDim: number, outDim: number): Float32Array {
    const output = new Float32Array(N * outDim);
    for (let n = 0; n < N; n++) {
      const inOffset = n * inDim;
      const outOffset = n * outDim;
      for (let o = 0; o < outDim; o++) {
        let sum = bias[o];
        for (let i = 0; i < inDim; i++) {
          sum += input[inOffset + i] * weight[o * inDim + i];
        }
        output[outOffset + o] = sum;
      }
    }
    return output;
  }

  /**
   * Batch linear + ReLU
   */
  private batchLinearRelu(input: Float32Array, weight: Float32Array, bias: Float32Array, N: number, inDim: number, outDim: number): Float32Array {
    const output = this.batchLinear(input, weight, bias, N, inDim, outDim);
    for (let i = 0; i < output.length; i++) {
      output[i] = Math.max(0, output[i]);
    }
    return output;
  }

  /**
   * Analyze array statistics
   */
  private analyzeArray(arr: Float32Array): { min: number; max: number; unique: number } {
    let min = Infinity, max = -Infinity;
    for (let i = 0; i < arr.length; i++) {
      if (arr[i] < min) min = arr[i];
      if (arr[i] > max) max = arr[i];
    }
    const sampleSize = Math.min(1000, arr.length);
    const uniqueCount = new Set(Array.from(arr.slice(0, sampleSize))).size;
    return { min, max, unique: uniqueCount };
  }

  /**
   * Cleanup resources
   */
  dispose(): void {
    this.weightBuffers.forEach(buffer => buffer.destroy());
    this.weightBuffers.clear();
    this.weights = null;
    this.initialized = false;
    this.callCount = 0;
    console.log('[TemplateDecoderWebGPU] Disposed');
  }
}

export default TemplateDecoderWebGPU;
