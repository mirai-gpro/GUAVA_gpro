// gvrm.ts
// ä¿®æ­£ç‰ˆ v73: TemplateDecoderWebGPUå¯¾å¿œ + scaleã‚¯ãƒ©ãƒ³ãƒ—ä¿®æ­£
// - RFDN Refiner (178KBè»½é‡ãƒ¢ãƒ‡ãƒ«)
// - TemplateDecoderWebGPU (scaleã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼ä¿®æ­£æ¸ˆã¿)

import { ImageEncoder } from './image-encoder';
import { InverseTextureMapper, type EHMMeshData, type ImageFeatures } from './inverse-texture-mapping';
import TemplateDecoderWebGPU, {
  type TemplateDecoderInput,
  type TemplateDecoderOutput
} from './template-decoder-webgpu';
import { UVDecoder } from './uv-decoder';
import { RFDNRefiner } from './rfdn-refiner-webgpu';  // â† æ–°ã—ã„è»½é‡Refiner
import { WebGLDisplay } from './webgl-display';
import { GuavaWebGPURendererPractical } from './guava-webgpu-renderer-practical';
import { GuavaWebGPURendererCompute } from './guava-webgpu-renderer-compute';
import { CameraUtils } from './camera-utils';

interface GVRMConfig {
  templatePath?: string;
  imagePath?: string;
  container?: HTMLElement;
  useWebGPURefiner?: boolean;  // WebGPU for refiner
}

interface GaussianData {
  positions: Float32Array;
  latents: Float32Array;
  opacity: Float32Array;
  scale: Float32Array;
  rotation: Float32Array;
  vertexCount: number;
}

interface PLYData {
  positions: Float32Array;
  vertexCount: number;
  faces: Uint32Array;
  faceCount: number;
  normals?: Float32Array;
  colors?: Float32Array;
}

/**
 * GVRM - GUAVA Virtual Reality Model
 * 
 * v72: RFDN Refinerå¯¾å¿œç‰ˆ
 * - è’¸ç•™æ¸ˆã¿è»½é‡Neural Refiner (178KB, å…ƒã®630å€åœ§ç¸®)
 * - idEmbeddingä¸è¦ (32chå…¥åŠ›ã®ã¿)
 * 
 * Pipeline:
 * 1. Image Encoder: Image â†’ Features (projection + ID embedding)
 * 2. Template Decoder: 3å…¥åŠ› (projection, base, id) â†’ Template Gaussians
 * 3. Inverse Texture Mapping: Image â†’ UV features (optional)
 * 4. UV Decoder: UV features â†’ UV Gaussians (optional)
 * 5. WebGPU Rendering: Gaussians â†’ Coarse feature map (32ch)
 * 6. RFDN Refiner: Coarse 32ch â†’ Refined RGB (idEmbä¸è¦ï¼)
 */
export class GVRM {
  // Asset paths
  private templatePath: string = '/assets/avatar_web.ply';
  private imagePath: string = '/assets/source.png';
  private cameraConfigPath: string = '/assets/source_camera.json';
  private uvCoordsPath: string = '/assets/uv_coords.bin';
  private container: HTMLElement | null = null;
  private useWebGPURefiner: boolean = true;
  
  // Core modules
  private imageEncoder: ImageEncoder;
  private templateDecoder: TemplateDecoderWebGPU | null = null;
  private inverseTextureMapper: InverseTextureMapper;
  private uvDecoder: UVDecoder;
  private neuralRefiner: RFDNRefiner;  // â† RFDN (è»½é‡ç‰ˆ)
  private webglDisplay: WebGLDisplay | null = null;
  
  // Data
  private plyData: PLYData | null = null;
  private uvCoords: Float32Array | null = null;
  private uvMappingData: EHMMeshData | null = null;
  private templateGaussians: GaussianData | null = null;
  private uvGaussians: GaussianData | null = null;
  private visibilityMask: Uint8Array | null = null;  // Tracks which vertices are visible in source image
  // idEmbeddingã¯ä¸è¦ã«ãªã£ãŸï¼
  
  // WebGPU
  private gpuDevice: GPUDevice | null = null;
  private gsCoarseRenderer: GuavaWebGPURendererPractical | null = null;
  private gsComputeRenderer: GuavaWebGPURendererCompute | null = null;
  private useComputeRenderer: boolean = true;  // â† 32ãƒãƒ£ãƒ³ãƒãƒ«å®Œå…¨ä¿æŒã®ãŸã‚Compute Rendererã‚’ä½¿ç”¨
  private debugBypassRFDN: boolean = false;  // DEBUG OFF: Use RFDN to convert 32ch â†’ RGB
  private readbackBuffers: GPUBuffer[] = [];
  private coarseFeatureArray: Float32Array | null = null;
  
  // State
  private initialized: boolean = false;
  private isRunning: boolean = false;
  private frameId: number | null = null;
  private frameCount: number = 0;
  
  constructor(config?: GVRMConfig) {
    if (config) {
      if (config.templatePath) this.templatePath = config.templatePath;
      if (config.imagePath) this.imagePath = config.imagePath;
      if (config.container) this.container = config.container;
      if (config.useWebGPURefiner !== undefined) this.useWebGPURefiner = config.useWebGPURefiner;
    }
    
    this.imageEncoder = new ImageEncoder();
    // templateDecoderã¯é‡ã¿ãƒ­ãƒ¼ãƒ‰å¾Œã«åˆæœŸåŒ–
    this.inverseTextureMapper = new InverseTextureMapper(512);
    this.uvDecoder = new UVDecoder();
    
    // Neural Refiner: RFDN (178KB distilled model)
    // StyleUNet export is problematic, using RFDN for now
    const useStyleUNet = false;  // â† RFDNã‚’ä½¿ç”¨ï¼ˆStyleUNetã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå•é¡Œï¼‰
    this.neuralRefiner = new RFDNRefiner({
      modelPath: useStyleUNet ? '/assets/styleunet_refiner.onnx' : '/assets/rfdn_refiner.onnx',
      useWebGPU: false  // WASMä½¿ç”¨ï¼ˆå®‰å®šæ€§å„ªå…ˆï¼‰
    });
    
    console.log('[GVRM] Created (v72: RFDN Refiner)');
  }
  
  async init(config?: GVRMConfig): Promise<void> {
    if (this.initialized) {
      console.warn('[GVRM] Already initialized');
      return;
    }
    
    try {
      console.log('[GVRM] ğŸš€ Initializing pipeline...');
      console.log('[GVRM] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
      console.log('[GVRM] ğŸ“¦ Using RFDN Refiner (178KB, 630x smaller)');
      
      // 1. WebGPU setup
      console.log('[GVRM] Step 1/6: WebGPU initialization');
      if (!navigator.gpu) throw new Error('WebGPU not supported');
      const adapter = await navigator.gpu.requestAdapter();
      if (!adapter) throw new Error('No GPU adapter');
      
      // 8ã¤ã®RGBA16Floatãƒ†ã‚¯ã‚¹ãƒãƒ£ç”¨ã«åˆ¶é™ã‚’ä¸Šã’ã‚‹
      const requiredLimits: GPUDeviceLimits = {};
      const adapterLimits = adapter.limits;
      
      // maxColorAttachmentBytesPerSample: 8 * RGBA16Float = 8 * 8 = 64 byteså¿…è¦
      if (adapterLimits.maxColorAttachmentBytesPerSample >= 128) {
        (requiredLimits as any).maxColorAttachmentBytesPerSample = 128;
        console.log('[GVRM]   Requesting maxColorAttachmentBytesPerSample: 128');
      } else if (adapterLimits.maxColorAttachmentBytesPerSample >= 64) {
        (requiredLimits as any).maxColorAttachmentBytesPerSample = 64;
        console.log('[GVRM]   Requesting maxColorAttachmentBytesPerSample: 64');
      }
      
      this.gpuDevice = await adapter.requestDevice({
        requiredLimits: requiredLimits as any
      });
      this.initReadbackBuffers(512, 512);
      console.log('[GVRM]   âœ… WebGPU ready');
      
      // 2. Display setup
      console.log('[GVRM] Step 2/6: Display setup');
      if (!this.container) {
        const autoContainer = document.getElementById('avatar3DContainer');
        if (autoContainer) this.container = autoContainer;
      }
      if (this.container) {
        this.webglDisplay = new WebGLDisplay(this.container, 512, 512);
        console.log('[GVRM]   âœ… Display ready');
      } else {
        console.warn('[GVRM]   âš ï¸ No container (headless mode)');
      }
      
      // 3. Load assets
      console.log('[GVRM] Step 3/6: Loading assets');
      const plyData = await this.loadPLY(this.templatePath);
      this.plyData = plyData;
      console.log(`[GVRM]   âœ… PLY loaded: ${plyData.vertexCount} vertices, ${plyData.faceCount} faces`);
      
      try {
        this.uvCoords = await this.loadBinary(this.uvCoordsPath);
        console.log(`[GVRM]   âœ… UV coords loaded: ${this.uvCoords.length / 2} vertices`);
      } catch (e) {
        console.warn('[GVRM]   âš ï¸ UV coords not found (template-only mode)');
      }
      
      // 4. Initialize modules
      console.log('[GVRM] Step 4/6: Initializing modules');
      
      await Promise.all([
        this.imageEncoder.init(),
        this.uvDecoder.init('/assets'),
        this.neuralRefiner.init()  // RFDN (178KB) - è¶…é«˜é€Ÿãƒ­ãƒ¼ãƒ‰
      ]);
      
      // Template Decoder initialization (WebGPU)
      this.templateDecoder = new TemplateDecoderWebGPU();
      await this.templateDecoder.init(this.gpuDevice!, '/assets');
      
      console.log('[GVRM]   âœ… All modules initialized');
      console.log('[GVRM]   ğŸ“Š RFDN Refiner: 178KB loaded (vs 107MB original)');
      
      // 5. Run inference pipeline
      console.log('[GVRM] Step 5/6: Running inference pipeline');
      await this.runInferencePipeline();
      console.log('[GVRM]   âœ… Inference complete');
      
      // 6. Setup renderer
      console.log('[GVRM] Step 6/6: Setting up renderer');
      await this.setupRenderer();
      console.log('[GVRM]   âœ… Renderer ready');
      
      this.initialized = true;
      this.isRunning = true;
      
      console.log('[GVRM] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
      console.log('[GVRM] âœ… Initialization complete!');
      console.log('[GVRM]   Template Gaussians: ', this.templateGaussians?.vertexCount || 0);
      console.log('[GVRM]   UV Gaussians: ', this.uvGaussians?.vertexCount || 0);
      console.log('[GVRM]   Total Gaussians: ', 
        (this.templateGaussians?.vertexCount || 0) + (this.uvGaussians?.vertexCount || 0));
      console.log('[GVRM]   ğŸš€ RFDN Refiner: No idEmbedding needed!');
      
      this.renderFrame();
      
    } catch (error) {
      console.error('[GVRM] âŒ Initialization failed:', error);
      throw error;
    }
  }
  
  /**
   * Run the inference pipeline (3å…¥åŠ›Template Decoderå¯¾å¿œç‰ˆ)
   * Note: idEmbeddingã¯Neural Refinerã§ä½¿ã‚ãªã„ãŒã€Template Decoderã§ã¯ä½¿ç”¨
   */
  private async runInferencePipeline(): Promise<void> {
    console.log('[GVRM] â”€â”€â”€ Inference Pipeline â”€â”€â”€');
    
    const vertexCount = this.uvCoords ? (this.uvCoords.length / 2) : 
                       (this.plyData!.vertexCount);
    
    console.log(`[GVRM] Using vertex count: ${vertexCount}`);
    
    // Prepare vertices
    const vertices = new Float32Array(vertexCount * 3);
    if (this.plyData!.positions.length >= vertexCount * 3) {
      vertices.set(this.plyData!.positions.subarray(0, vertexCount * 3));
    } else {
      vertices.set(this.plyData!.positions);
    }
    
    // ===== PHASE 1: Image Encoding =====
    console.log('[GVRM] Phase 1: Image encoding');
    console.log(`[GVRM]   Input image: ${this.imagePath}`);
    console.log(`[GVRM]   Vertices: ${vertexCount}`);
    
    const { projectionFeature, idEmbedding, visibilityMask } =
      await this.imageEncoder.extractFeaturesWithSourceCamera(
        this.imagePath,
        {},
        vertices,
        vertexCount,
        128
      );

    // Store visibility mask for opacity masking
    this.visibilityMask = visibilityMask;
    
    console.log('[GVRM]   âœ… Encoder output:');
    console.log(`[GVRM]      Projection features: [${vertexCount}, 128]`);
    const projStats = this.analyzeArray(projectionFeature);
    console.log(`[GVRM]        stats: min=${projStats.min.toFixed(4)}, max=${projStats.max.toFixed(4)}, nonZeros=${projStats.nonZeros}`);
    console.log(`[GVRM]      ID embedding (CLS token): [${idEmbedding.length}]`);
    const idStats = this.analyzeArray(idEmbedding);
    console.log(`[GVRM]        stats: min=${idStats.min.toFixed(4)}, max=${idStats.max.toFixed(4)}, nonZeros=${idStats.nonZeros}`);
    
    // ===== PHASE 2: Template Gaussian Decoding (å®Œå…¨ç‰ˆONNX) =====
    console.log('[GVRM] Phase 2: Template Gaussian decoding');
    
    if (!this.templateDecoder) {
      throw new Error('Template Decoder not initialized');
    }
    
    // å…¥åŠ›æº–å‚™ï¼ˆæ–°ã—ã„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼šglobal_embeddingã¯768æ¬¡å…ƒï¼‰
    const decoderInput: TemplateDecoderInput = {
      projection_features: projectionFeature,  // [N, 128]
      global_embedding: idEmbedding,           // [768] â† CLS tokenç›´æ¥
      num_vertices: vertexCount
    };
    
    console.log('[GVRM]   Input validation:');
    console.log(`[GVRM]     projection_features: [${vertexCount}, 128]`);
    console.log(`[GVRM]     global_embedding: [${idEmbedding.length}] (CLS token)`);
    console.log(`[GVRM]     num_vertices: ${vertexCount}`);
    console.log(`[GVRM]   Note: base_features and global_mapping are embedded in ONNX`);
    
    const projInputStats = this.analyzeArray(projectionFeature);
    const idInputStats = this.analyzeArray(idEmbedding);
    console.log(`[GVRM]     Projection stats: min=${projInputStats.min.toFixed(4)}, max=${projInputStats.max.toFixed(4)}, nonZeros=${projInputStats.nonZeros}`);
    console.log(`[GVRM]     Global embedding stats: min=${idInputStats.min.toFixed(4)}, max=${idInputStats.max.toFixed(4)}, nonZeros=${idInputStats.nonZeros}`);
    
    // ONNX Template Decoderã§æ¨è«–å®Ÿè¡Œ
    console.log('[GVRM]   Running Complete Template Decoder (ONNX)...');
    const templateOutput: TemplateDecoderOutput = await this.templateDecoder.forward(decoderInput);
    
    // Set positions from vertices (decoder doesn't modify positions)
    templateOutput.positions = vertices;

    // Note: idEmbeddingã¯RFDN Refinerã§ã¯ä¸è¦ã«ãªã£ãŸï¼
    // this.idEmbedding = templateOutput.id_embedding;  // â† å‰Šé™¤

    // ================================================================
    // é‡è¦: ç”»åƒç¯„å›²å¤–ã®é ‚ç‚¹ã®opacityã‚’ã‚¼ãƒ­ã«ã™ã‚‹
    // Template Decoderã¯ base_features ã¨ global_embedding ã‹ã‚‰
    // ç¯„å›²å¤–é ‚ç‚¹ã«ã‚‚éã‚¼ãƒ­ã®opacityã‚’å‡ºåŠ›ã™ã‚‹ãŸã‚ã€æ˜ç¤ºçš„ã«ãƒã‚¹ã‚¯
    // ================================================================
    let invisibleCount = 0;
    if (this.visibilityMask) {
      for (let i = 0; i < vertexCount; i++) {
        if (this.visibilityMask[i] === 0) {
          templateOutput.opacities[i] = 0;
          invisibleCount++;
        }
      }
      console.log(`[GVRM] âš ï¸ Opacity masked: ${invisibleCount}/${vertexCount} out-of-bounds vertices set to opacity=0`);
    }

    this.templateGaussians = {
      positions: templateOutput.positions,  // [N, 3]
      latents: templateOutput.colors,       // [N, 32]
      opacity: templateOutput.opacities,    // [N, 1]
      scale: templateOutput.scales,         // [N, 3]
      rotation: templateOutput.rotations,   // [N, 4]
      vertexCount: vertexCount
    };
    
    console.log('[GVRM]   âœ… Template Gaussians generated (ONNX)');
    console.log(`[GVRM]      Count: ${vertexCount}`);
    console.log(`[GVRM]      Positions: [${vertexCount}, 3]`);
    console.log(`[GVRM]      Colors (latent): [${vertexCount}, 32]`);
    console.log(`[GVRM]      Opacities: [${vertexCount}, 1]`);
    console.log(`[GVRM]      Scales: [${vertexCount}, 3]`);
    console.log(`[GVRM]      Rotations: [${vertexCount}, 4]`);
    
    // Outputçµ±è¨ˆ
    const opacityStats = this.analyzeArray(templateOutput.opacities);
    const scaleStats = this.analyzeArray(templateOutput.scales);
    const colorStats = this.analyzeArray(templateOutput.colors);
    const rotationStats = this.analyzeArray(templateOutput.rotations);
    console.log(`[GVRM]      Opacity stats: min=${opacityStats.min.toFixed(4)}, max=${opacityStats.max.toFixed(4)}`);
    console.log(`[GVRM]      Scale stats: min=${scaleStats.min.toFixed(4)}, max=${scaleStats.max.toFixed(4)}`);
    console.log(`[GVRM]      Color stats: min=${colorStats.min.toFixed(4)}, max=${colorStats.max.toFixed(4)}`);
    console.log(`[GVRM]      Rotation stats: min=${rotationStats.min.toFixed(4)}, max=${rotationStats.max.toFixed(4)}`);
    
    // NaN/Infinity ãƒã‚§ãƒƒã‚¯
    let nanCount = 0, infCount = 0;
    for (let i = 0; i < templateOutput.opacities.length; i++) {
      if (isNaN(templateOutput.opacities[i])) nanCount++;
      if (!isFinite(templateOutput.opacities[i])) infCount++;
    }
    if (nanCount > 0 || infCount > 0) {
      console.error(`[GVRM] âŒ Template Decoder output contains invalid values!`);
      console.error(`[GVRM]    NaN count: ${nanCount}, Inf count: ${infCount}`);
    }
    
    // ===== PHASE 3: UV Pipeline (Optional) =====
    if (this.uvMappingData) {
      console.log('[GVRM] Phase 3: UV pipeline');
      console.log('[GVRM]   âš ï¸ UV pipeline currently disabled (requires EHM mesh data)');
    } else {
      console.log('[GVRM] Phase 3: UV pipeline skipped (no UV mapping data)');
    }
    
    console.log('[GVRM] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€');
  }
  
  /**
   * Setup WebGPU renderer with canonical camera
   */
  private async setupRenderer(): Promise<void> {
    if (!this.templateGaussians || !this.gpuDevice) {
      throw new Error('Missing data for renderer setup');
    }
    
    let finalGaussians: GaussianData;
    
    if (this.uvGaussians) {
      const totalCount = this.templateGaussians.vertexCount + this.uvGaussians.vertexCount;
      
      finalGaussians = {
        positions: new Float32Array(totalCount * 3),
        latents: new Float32Array(totalCount * 32),
        opacity: new Float32Array(totalCount),
        scale: new Float32Array(totalCount * 3),
        rotation: new Float32Array(totalCount * 4),
        vertexCount: totalCount
      };
      
      finalGaussians.positions.set(this.templateGaussians.positions);
      finalGaussians.latents.set(this.templateGaussians.latents);
      finalGaussians.opacity.set(this.templateGaussians.opacity);
      finalGaussians.scale.set(this.templateGaussians.scale);
      finalGaussians.rotation.set(this.templateGaussians.rotation);
      
      const tCount = this.templateGaussians.vertexCount;
      finalGaussians.positions.set(this.uvGaussians.positions, tCount * 3);
      finalGaussians.latents.set(this.uvGaussians.latents, tCount * 32);
      finalGaussians.opacity.set(this.uvGaussians.opacity, tCount);
      finalGaussians.scale.set(this.uvGaussians.scale, tCount * 3);
      finalGaussians.rotation.set(this.uvGaussians.rotation, tCount * 4);
      
      console.log('[GVRM] Merged Gaussians:', {
        template: this.templateGaussians.vertexCount,
        uv: this.uvGaussians.vertexCount,
        total: totalCount
      });
    } else {
      finalGaussians = this.templateGaussians;
      console.log('[GVRM] Using template Gaussians only:', finalGaussians.vertexCount);
    }
    
    const viewMatrix = CameraUtils.getCanonicalViewMatrix();
    const projMatrix = CameraUtils.getProjMatrix(1.0);

    const cameraConfig = {
      viewMatrix: viewMatrix,
      projMatrix: projMatrix,
      imageWidth: 512,
      imageHeight: 512
    };

    if (this.useComputeRenderer) {
      // Compute Renderer: 32ãƒãƒ£ãƒ³ãƒãƒ«å®Œå…¨ä¿æŒ (ãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒˆã‚·ã‚§ãƒ¼ãƒ€ãƒ¼ã®ã‚¢ãƒ«ãƒ•ã‚¡ãƒ–ãƒ¬ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã«ã‚ˆã‚‹ãƒãƒ£ãƒ³ãƒãƒ«æ¬ è½ã‚’å›é¿)
      this.gsComputeRenderer = new GuavaWebGPURendererCompute(
        this.gpuDevice,
        finalGaussians,
        cameraConfig
      );
      await this.gsComputeRenderer.waitForInit();
      console.log('[GVRM] âœ… Compute Renderer configured (32 channels preserved)');
    } else {
      // Practical Renderer: å¾“æ¥ã®ãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒˆã‚·ã‚§ãƒ¼ãƒ€ãƒ¼æ–¹å¼ (8ãƒãƒ£ãƒ³ãƒãƒ«æ¬ è½)
      this.gsCoarseRenderer = new GuavaWebGPURendererPractical(
        this.gpuDevice,
        finalGaussians,
        cameraConfig
      );
      console.log('[GVRM] Practical Renderer configured (warning: 8 channels lost to alpha blending)');
    }
  }
  
  /**
   * Render loop (RFDN Refinerç‰ˆ - idEmbeddingä¸è¦)
   * ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¬ãƒ¼ãƒˆåˆ¶é™: GPUãƒãƒ³ã‚°å¯¾ç­–
   */
  private lastRenderTime: number = 0;
  private readonly MIN_FRAME_INTERVAL = 500; // æœ€ä½500msé–“éš” (2fps)
  
  private renderFrame = async (): Promise<void> => {
    if (!this.isRunning || !this.initialized) return;
    
    // ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¬ãƒ¼ãƒˆåˆ¶é™
    const now = performance.now();
    if (now - this.lastRenderTime < this.MIN_FRAME_INTERVAL) {
      this.frameId = requestAnimationFrame(this.renderFrame);
      return;
    }
    this.lastRenderTime = now;
    
    try {
      this.frameCount++;

      let coarseFeatures: Float32Array;

      if (this.useComputeRenderer && this.gsComputeRenderer) {
        // Compute Renderer: 32ãƒãƒ£ãƒ³ãƒãƒ«å®Œå…¨ä¿æŒ
        this.gsComputeRenderer.sort();
        this.gsComputeRenderer.render();

        const outputBuffers = this.gsComputeRenderer.getOutputBuffers();
        coarseFeatures = await this.convertBuffersToFloat32Array(outputBuffers);

        if (this.frameCount === 1) {
          console.log('[GVRM] ğŸš€ Using Compute Renderer (all 32 channels preserved)');
        }
      } else if (this.gsCoarseRenderer) {
        // Practical Renderer: å¾“æ¥æ–¹å¼ (8ãƒãƒ£ãƒ³ãƒãƒ«æ¬ è½ã‚ã‚Š)
        this.gsCoarseRenderer.sort();
        this.gsCoarseRenderer.render();

        const outputTextures = this.gsCoarseRenderer.getOutputTextures();
        coarseFeatures = await this.convertTexturesToFloat32Array(outputTextures);

        if (this.frameCount === 1) {
          console.log('[GVRM] âš ï¸ Using Practical Renderer (8 channels interpolated)');
        }
      } else {
        throw new Error('No renderer available');
      }

      let displayRGB: Float32Array;

      if (this.debugBypassRFDN) {
        // DEBUG: RFDNã‚’ãƒã‚¤ãƒ‘ã‚¹ã—ã¦æœ€åˆ3ãƒãƒ£ãƒ³ãƒãƒ«ã‚’RGBã¨ã—ã¦ç›´æ¥è¡¨ç¤º
        // ã“ã‚Œã«ã‚ˆã‚Šã€Gaussian splattingè‡ªä½“ãŒæ­£ã—ãå‹•ä½œã—ã¦ã„ã‚‹ã‹ã‚’ç¢ºèª
        const width = 512, height = 512;
        displayRGB = new Float32Array(width * height * 3);

        // æœ€åˆ3ãƒãƒ£ãƒ³ãƒãƒ«ã®çµ±è¨ˆã‚’å–å¾—ï¼ˆæ­£è¦åŒ–ç”¨ï¼‰
        const pixelCount = width * height;
        let minVal = Infinity, maxVal = -Infinity;
        for (let ch = 0; ch < 3; ch++) {
          for (let p = 0; p < pixelCount; p++) {
            const val = coarseFeatures[ch * pixelCount + p];
            if (val < minVal) minVal = val;
            if (val > maxVal) maxVal = val;
          }
        }
        const range = maxVal - minVal || 1;

        // CHW â†’ HWCå¤‰æ› + æ­£è¦åŒ– [0, 1]
        for (let y = 0; y < height; y++) {
          for (let x = 0; x < width; x++) {
            const p = y * width + x;
            for (let c = 0; c < 3; c++) {
              const srcIdx = c * pixelCount + p;
              const dstIdx = p * 3 + c;
              // æ­£è¦åŒ–: [minVal, maxVal] â†’ [0, 1]
              displayRGB[dstIdx] = (coarseFeatures[srcIdx] - minVal) / range;
            }
          }
        }

        if (this.frameCount === 1) {
          console.log('[GVRM] ğŸ”§ DEBUG: Bypassing RFDN, showing first 3 channels as RGB');
          console.log(`[GVRM]   Normalization: [${minVal.toFixed(4)}, ${maxVal.toFixed(4)}] â†’ [0, 1]`);
        }
      } else {
        // RFDN Refiner: idEmbeddingä¸è¦ï¼32chç‰¹å¾´ãƒãƒƒãƒ—ã®ã¿
        displayRGB = await this.neuralRefiner.process(coarseFeatures);
      }

      if (this.webglDisplay) {
        this.webglDisplay.display(displayRGB, this.frameCount);
      }

      if (this.frameCount === 1) {
        const coarseStats = this.analyzeArray(coarseFeatures.slice(0, 10000));
        const displayStats = this.analyzeArray(displayRGB.slice(0, 10000));
        console.log('[GVRM] First frame stats:');
        console.log(`  Coarse features (32ch): min=${coarseStats.min.toFixed(4)}, max=${coarseStats.max.toFixed(4)}`);
        console.log(`  Display RGB: min=${displayStats.min.toFixed(4)}, max=${displayStats.max.toFixed(4)}`);
        if (!this.debugBypassRFDN) {
          console.log(`  ğŸš€ RFDN Refiner: No idEmbedding used (178KB model)`);
        }
      }

    } catch (error) {
      console.error('[GVRM] Render error:', error);
      this.isRunning = false;
      return;
    }
    
    this.frameId = requestAnimationFrame(this.renderFrame);
  };
  
  // ===== Helper Methods =====
  
  private initReadbackBuffers(width: number, height: number): void {
    if (!this.gpuDevice) return;
    
    this.readbackBuffers.forEach(b => b.destroy());
    this.readbackBuffers = [];
    
    const bytesPerRow = Math.ceil((width * 8) / 256) * 256;
    
    for (let i = 0; i < 8; i++) {
      this.readbackBuffers.push(this.gpuDevice.createBuffer({
        size: bytesPerRow * height,
        usage: GPUBufferUsage.COPY_DST | GPUBufferUsage.MAP_READ
      }));
    }
    
    this.coarseFeatureArray = new Float32Array(width * height * 32);
  }
  
  private async convertTexturesToFloat32Array(textures: GPUTexture[]): Promise<Float32Array> {
    if (!this.gpuDevice || !this.coarseFeatureArray) {
      throw new Error('Buffers not initialized');
    }
    
    const width = 512, height = 512;
    const bytesPerRow = Math.ceil((width * 8) / 256) * 256;
    
    const commandEncoder = this.gpuDevice.createCommandEncoder();
    for (let i = 0; i < 8; i++) {
      commandEncoder.copyTextureToBuffer(
        { texture: textures[i] },
        { buffer: this.readbackBuffers[i], bytesPerRow: bytesPerRow },
        [width, height]
      );
    }
    this.gpuDevice.queue.submit([commandEncoder.finish()]);
    
    await Promise.all(this.readbackBuffers.map(buf => buf.mapAsync(GPUMapMode.READ)));
    
    // Debug: å„MRTã®çµ±è¨ˆ
    const mrtStats: string[] = [];
    
    for (let i = 0; i < 8; i++) {
      const mappedRange = this.readbackBuffers[i].getMappedRange();
      const source = new Uint16Array(mappedRange);
      
      // Debug: æœ€åˆã®MRTã®ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«
      if (i === 0 && this.frameCount === 1) {
        console.log('[GVRM] MRT0 raw Uint16 samples:', Array.from(source.slice(0, 20)));
      }
      
      let mrtMin = Infinity, mrtMax = -Infinity, mrtNaN = 0, mrtInf = 0;
      
      for (let y = 0; y < height; y++) {
        for (let x = 0; x < width; x++) {
          for (let c = 0; c < 4; c++) {
            const srcIdx = y * (bytesPerRow / 2) + x * 4 + c;
            const dstIdx = (i * 4 + c) * width * height + y * width + x;
            const val = this.float16ToFloat32(source[srcIdx]);
            this.coarseFeatureArray[dstIdx] = val;
            
            if (isNaN(val)) mrtNaN++;
            else if (!isFinite(val)) mrtInf++;
            else {
              if (val < mrtMin) mrtMin = val;
              if (val > mrtMax) mrtMax = val;
            }
          }
        }
      }
      
      if (this.frameCount === 1) {
        mrtStats.push(`MRT${i}: [${mrtMin.toFixed(2)}, ${mrtMax.toFixed(2)}] NaN=${mrtNaN} Inf=${mrtInf}`);
      }

      this.readbackBuffers[i].unmap();
    }

    // ================================================================
    // å¿œæ€¥å‡¦ç½®: ã‚¢ãƒ«ãƒ•ã‚¡ãƒãƒ£ãƒ³ãƒãƒ«ã«ä¸Šæ›¸ãã•ã‚ŒãŸæ¬ è½ãƒãƒ£ãƒ³ãƒãƒ«ã‚’è£œé–“
    // MRTã®4ç•ªç›®ã®ãƒãƒ£ãƒ³ãƒãƒ«(A)ã¯blendingç”¨alphaãªã®ã§ã€
    // å®Ÿéš›ã®latentãƒãƒ£ãƒ³ãƒãƒ«3,7,11,15,19,23,27,31ãŒæ¬ è½ã—ã¦ã„ã‚‹
    // éš£æ¥ãƒãƒ£ãƒ³ãƒãƒ«ã‹ã‚‰ã‚³ãƒ”ãƒ¼ã—ã¦è£œé–“
    // ================================================================
    const pixelCount = width * height;
    const missingChannels = [3, 7, 11, 15, 19, 23, 27, 31];
    for (const ch of missingChannels) {
      const srcCh = ch - 1;  // éš£æ¥ãƒãƒ£ãƒ³ãƒãƒ«ã‹ã‚‰ã‚³ãƒ”ãƒ¼
      const srcOffset = srcCh * pixelCount;
      const dstOffset = ch * pixelCount;
      for (let p = 0; p < pixelCount; p++) {
        this.coarseFeatureArray[dstOffset + p] = this.coarseFeatureArray[srcOffset + p];
      }
    }

    if (this.frameCount === 1) {
      console.log('[GVRM] âš ï¸ Missing channel fix applied: ch 3,7,11,15,19,23,27,31 interpolated from adjacent');
    }
    
    if (this.frameCount === 1) {
      console.log('[GVRM] MRT readback stats:');
      mrtStats.forEach(s => console.log('  ' + s));
    }
    
    return this.coarseFeatureArray;
  }
  
  /**
   * Compute Rendererç”¨: ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ãƒãƒƒãƒ•ã‚¡ã‹ã‚‰32ãƒãƒ£ãƒ³ãƒãƒ«ã‚’èª­ã¿å‡ºã—
   * Practical Rendererã¨ç•°ãªã‚Šã€å…¨32ãƒãƒ£ãƒ³ãƒãƒ«ãŒæ­£ã—ãä¿æŒã•ã‚Œã¦ã„ã‚‹
   */
  private async convertBuffersToFloat32Array(buffers: GPUBuffer[]): Promise<Float32Array> {
    if (!this.gpuDevice) {
      throw new Error('GPU device not initialized');
    }

    const width = 512, height = 512;
    const pixelCount = width * height;

    // å‡ºåŠ›é…åˆ—ã‚’åˆæœŸåŒ–
    if (!this.coarseFeatureArray || this.coarseFeatureArray.length !== pixelCount * 32) {
      this.coarseFeatureArray = new Float32Array(pixelCount * 32);
    }

    // èª­ã¿å‡ºã—ç”¨ãƒãƒƒãƒ•ã‚¡ã‚’ä½œæˆ
    const readbackSize = pixelCount * 4 * 4;  // 4 floats x 4 bytes
    const readbackBuffers: GPUBuffer[] = [];

    const commandEncoder = this.gpuDevice.createCommandEncoder();
    for (let i = 0; i < 8; i++) {
      const readback = this.gpuDevice.createBuffer({
        size: readbackSize,
        usage: GPUBufferUsage.COPY_DST | GPUBufferUsage.MAP_READ
      });
      commandEncoder.copyBufferToBuffer(buffers[i], 0, readback, 0, readbackSize);
      readbackBuffers.push(readback);
    }
    this.gpuDevice.queue.submit([commandEncoder.finish()]);

    // éåŒæœŸã§èª­ã¿å‡ºã—
    await Promise.all(readbackBuffers.map(buf => buf.mapAsync(GPUMapMode.READ)));

    // Debug: çµ±è¨ˆæƒ…å ±
    const bufStats: string[] = [];

    for (let i = 0; i < 8; i++) {
      const mappedRange = readbackBuffers[i].getMappedRange();
      const source = new Float32Array(mappedRange);

      let bufMin = Infinity, bufMax = -Infinity, bufNaN = 0;

      for (let p = 0; p < pixelCount; p++) {
        for (let c = 0; c < 4; c++) {
          const srcIdx = p * 4 + c;
          const channel = i * 4 + c;
          const dstIdx = channel * pixelCount + p;
          const val = source[srcIdx];
          this.coarseFeatureArray[dstIdx] = val;

          if (isNaN(val)) bufNaN++;
          else {
            if (val < bufMin) bufMin = val;
            if (val > bufMax) bufMax = val;
          }
        }
      }

      if (this.frameCount === 1) {
        bufStats.push(`Buf${i}: [${bufMin.toFixed(2)}, ${bufMax.toFixed(2)}] NaN=${bufNaN}`);
      }

      readbackBuffers[i].unmap();
      readbackBuffers[i].destroy();
    }

    if (this.frameCount === 1) {
      console.log('[GVRM] Compute Renderer buffer stats (32 channels, no loss):');
      bufStats.forEach(s => console.log('  ' + s));
    }

    return this.coarseFeatureArray;
  }

  private float16ToFloat32(f16: number): number {
    const sign = (f16 & 0x8000) >> 15;
    const exponent = (f16 & 0x7C00) >> 10;
    const mantissa = f16 & 0x03FF;
    
    if (exponent === 0) {
      return (sign ? -1 : 1) * Math.pow(2, -14) * (mantissa / 1024);
    }
    if (exponent === 31) {
      return mantissa ? NaN : (sign ? -Infinity : Infinity);
    }
    
    return (sign ? -1 : 1) * Math.pow(2, exponent - 15) * (1 + mantissa / 1024);
  }
  
  private async loadPLY(url: string): Promise<PLYData> {
    const response = await fetch(url);
    if (!response.ok) throw new Error(`Failed to load PLY: ${url}`);
    const buffer = await response.arrayBuffer();
    return this.parsePLY(buffer);
  }
  
  private parsePLY(buffer: ArrayBuffer): PLYData {
    const headerText = new TextDecoder('utf-8').decode(buffer.slice(0, 4000));
    const headerLines = headerText.split('\n');
    let vertexCount = 0, faceCount = 0, headerEnd = 0;
    let vertexProps: { name: string, type: string }[] = [];
    
    for (const line of headerLines) {
      const trimmed = line.trim();
      if (trimmed === 'end_header') {
        headerEnd += trimmed.length + 1;
        while (headerEnd < buffer.byteLength && new Uint8Array(buffer)[headerEnd] < 32) {
          headerEnd++;
        }
        break;
      }
      headerEnd += line.length + 1;
      
      if (trimmed.startsWith('element vertex')) {
        vertexCount = parseInt(trimmed.split(/\s+/)[2]);
      } else if (trimmed.startsWith('element face')) {
        faceCount = parseInt(trimmed.split(/\s+/)[2]);
      } else if (trimmed.startsWith('property')) {
        if (vertexCount > 0 && faceCount === 0) {
          const parts = trimmed.split(/\s+/);
          vertexProps.push({ type: parts[1], name: parts[2] });
        }
      }
    }
    
    const dataView = new DataView(buffer);
    let offset = headerEnd;
    const positions = new Float32Array(vertexCount * 3);
    const normals = new Float32Array(vertexCount * 3);
    const colors = new Float32Array(vertexCount * 3);
    
    for (let i = 0; i < vertexCount; i++) {
      for (const prop of vertexProps) {
        let val = 0;
        if (prop.type === 'float' || prop.type === 'float32') {
          val = dataView.getFloat32(offset, true);
          offset += 4;
        } else if (prop.type === 'uchar' || prop.type === 'uint8') {
          val = dataView.getUint8(offset);
          offset += 1;
        } else {
          offset += 4;
        }
        
        if (prop.name === 'x') positions[i * 3 + 0] = val;
        if (prop.name === 'y') positions[i * 3 + 1] = val;
        if (prop.name === 'z') positions[i * 3 + 2] = val;
        if (prop.name === 'nx') normals[i * 3 + 0] = val;
        if (prop.name === 'ny') normals[i * 3 + 1] = val;
        if (prop.name === 'nz') normals[i * 3 + 2] = val;
        if (prop.name === 'red') colors[i * 3 + 0] = val / 255.0;
        if (prop.name === 'green') colors[i * 3 + 1] = val / 255.0;
        if (prop.name === 'blue') colors[i * 3 + 2] = val / 255.0;
      }
    }
    
    const faces = new Uint32Array(faceCount * 3);
    for (let i = 0; i < faceCount; i++) {
      const numVerts = dataView.getUint8(offset);
      offset += 1;
      if (numVerts === 3) {
        faces[i * 3 + 0] = dataView.getUint32(offset, true); offset += 4;
        faces[i * 3 + 1] = dataView.getUint32(offset, true); offset += 4;
        faces[i * 3 + 2] = dataView.getUint32(offset, true); offset += 4;
      } else {
        offset += numVerts * 4;
      }
    }
    
    return { positions, vertexCount, faces, faceCount, normals, colors };
  }
  
  private async loadBinary(url: string): Promise<Float32Array> {
    const response = await fetch(url);
    if (!response.ok) throw new Error(`Failed to load binary: ${url}`);
    return new Float32Array(await response.arrayBuffer());
  }
  
  private analyzeArray(arr: Float32Array): { min: number; max: number; mean: number; nonZeros: number } {
    let min = Infinity, max = -Infinity, sum = 0, nonZeros = 0;
    for (let i = 0; i < arr.length; i++) {
      const v = arr[i];
      if (!isFinite(v)) continue;
      if (v < min) min = v;
      if (v > max) max = v;
      sum += v;
      if (Math.abs(v) > 0.001) nonZeros++;
    }
    return { min, max, mean: sum / arr.length, nonZeros };
  }
  
  dispose(): void {
    this.isRunning = false;
    if (this.frameId !== null) {
      cancelAnimationFrame(this.frameId);
      this.frameId = null;
    }
    
    this.readbackBuffers.forEach(b => b.destroy());
    this.gsCoarseRenderer?.destroy();
    this.webglDisplay?.dispose();
    this.imageEncoder.dispose();
    if (this.templateDecoder) {
      // Template Decoderã«disposeãƒ¡ã‚½ãƒƒãƒ‰ãŒã‚ã‚Œã°å‘¼ã³å‡ºã™
    }
    this.inverseTextureMapper.dispose();
    this.uvDecoder.dispose();
    this.neuralRefiner.dispose();
    
    console.log('[GVRM] Disposed');
  }
}
