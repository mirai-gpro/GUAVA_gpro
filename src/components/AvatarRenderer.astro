---
// AvatarRenderer.astro - 3DGS Avatar with GaussianSplats3D + VRM Lip Sync
// Based on gaussian-vrm architecture (GaussianSplats3D + three-vrm)
export interface Props {
  plyPath?: string;
  gvrmPath?: string;  // Future: GVRM format support
  quality?: 'low' | 'medium' | 'high';
}

const {
  plyPath = '/assets/avatar/GS_canonical.ply',
  gvrmPath = '',
  quality = 'medium'
} = Astro.props;

const qualitySettings = {
  low: { gaussians: 5000, splatScale: 0.8 },
  medium: { gaussians: 10000, splatScale: 1.0 },
  high: { gaussians: 50000, splatScale: 1.2 }
};
---

<div class="avatar-renderer" data-ply-path={plyPath} data-gvrm-path={gvrmPath} data-quality={quality}>
  <canvas id="avatarCanvas"></canvas>
  <div class="avatar-fallback" id="avatarFallback">
    <img src="/images/avatar-anime.png" alt="AI Avatar" class="fallback-img" />
  </div>
  <div class="avatar-status" id="avatarStatus">
    <span class="status-dot"></span>
    <span class="status-text">3DGS Loading...</span>
  </div>
</div>

<style>
.avatar-renderer {
  width: 100%;
  height: 100%;
  position: relative;
  display: flex;
  align-items: center;
  justify-content: center;
}

#avatarCanvas {
  width: 100%;
  height: 100%;
  position: absolute;
  top: 0;
  left: 0;
  z-index: 1;
}

.avatar-fallback {
  width: 110px;
  height: 110px;
  position: relative;
  z-index: 2;
  transition: opacity 0.5s ease;
}

.avatar-fallback.hidden {
  opacity: 0;
  pointer-events: none;
}

.fallback-img {
  width: 100%;
  height: 100%;
  object-fit: contain;
  filter: drop-shadow(0 4px 6px rgba(0,0,0,0.2));
}

.avatar-renderer.speaking .fallback-img {
  animation: breathe 1s infinite alternate ease-in-out;
  filter: drop-shadow(0 0 15px rgba(6, 182, 212, 0.6));
}

@keyframes breathe {
  0% { transform: scale(1); }
  100% { transform: scale(1.05) translateY(-2px); }
}

.avatar-status {
  position: absolute;
  bottom: 8px;
  left: 50%;
  transform: translateX(-50%);
  background: rgba(0, 0, 0, 0.6);
  padding: 4px 12px;
  border-radius: 12px;
  font-size: 10px;
  color: white;
  display: flex;
  align-items: center;
  gap: 6px;
  opacity: 0;
  transition: opacity 0.3s;
  z-index: 10;
}

.avatar-status.visible {
  opacity: 1;
}

.avatar-status.loaded {
  background: rgba(6, 182, 212, 0.8);
}

.avatar-status.error {
  background: rgba(239, 68, 68, 0.8);
}

.status-dot {
  width: 6px;
  height: 6px;
  border-radius: 50%;
  background: #fbbf24;
  animation: pulse 1.5s infinite;
}

.avatar-status.loaded .status-dot {
  background: #34d399;
  animation: none;
}

.avatar-status.error .status-dot {
  background: #f87171;
  animation: none;
}

@keyframes pulse {
  0%, 100% { opacity: 1; }
  50% { opacity: 0.5; }
}
</style>

<script>
import * as THREE from 'three';
import { OrbitControls } from 'three/examples/jsm/controls/OrbitControls.js';
import * as GaussianSplats3D from '@mkkellogg/gaussian-splats-3d';

/**
 * AvatarRenderer - 3DGS Avatar with GaussianSplats3D + VRM-style Lip Sync
 *
 * Architecture (based on gaussian-vrm):
 * - GaussianSplats3D: High-quality 3DGS rendering from PLY files
 * - VRM Blendshapes: a, i, u, e, o for lip sync (準備中)
 * - Audio frequency analysis for real-time lip sync
 *
 * Future: Full GVRM format support with skinned mesh binding
 */
class AvatarRenderer {
  private container: HTMLElement;
  private canvas: HTMLCanvasElement;
  private fallback: HTMLElement;
  private status: HTMLElement;
  private scene: THREE.Scene;
  private camera: THREE.PerspectiveCamera;
  private renderer: THREE.WebGLRenderer;
  private controls: OrbitControls;
  private viewer: any = null;  // GaussianSplats3D viewer
  private splatScene: any = null;
  private isLoaded: boolean = false;
  private isSpeaking: boolean = false;
  private animationFrameId: number = 0;
  private audioAnalyser: AnalyserNode | null = null;
  private audioContext: AudioContext | null = null;

  // VRM-style blendshape weights for lip sync
  // Viseme mapping: a, i, u, e, o (Japanese vowels)
  private visemeWeights = {
    a: 0, i: 0, u: 0, e: 0, o: 0,
    jawOpen: 0,
    blink: 0
  };

  // Lip sync parameters derived from audio
  private lipSyncParams = {
    jawOpen: 0,
    lipSync: 0,
    currentViseme: 'neutral' as 'a' | 'i' | 'u' | 'e' | 'o' | 'neutral'
  };

  constructor(container: HTMLElement) {
    this.container = container;
    this.canvas = container.querySelector('#avatarCanvas') as HTMLCanvasElement;
    this.fallback = container.querySelector('#avatarFallback') as HTMLElement;
    this.status = container.querySelector('#avatarStatus') as HTMLElement;

    this.scene = new THREE.Scene();
    this.camera = new THREE.PerspectiveCamera(45, 1, 0.1, 100);
    this.renderer = new THREE.WebGLRenderer({
      canvas: this.canvas,
      antialias: true,
      alpha: true
    });
    this.controls = new OrbitControls(this.camera, this.canvas);

    this.init();
  }

  private init() {
    const rect = this.container.getBoundingClientRect();
    this.renderer.setSize(rect.width, rect.height);
    this.renderer.setPixelRatio(Math.min(window.devicePixelRatio, 2));
    this.renderer.setClearColor(0x000000, 0);

    this.camera.position.set(0, 0, 2.5);
    this.camera.aspect = rect.width / rect.height;
    this.camera.updateProjectionMatrix();

    this.controls.enableDamping = true;
    this.controls.dampingFactor = 0.05;
    this.controls.enableZoom = true;
    this.controls.enablePan = false;
    this.controls.minDistance = 1;
    this.controls.maxDistance = 5;

    // Lighting for 3DGS
    const ambientLight = new THREE.AmbientLight(0xffffff, 1.0);
    this.scene.add(ambientLight);

    this.updateStatus('loading', '3DGS Loading...');
    this.loadGaussianAvatar();

    window.addEventListener('resize', () => this.handleResize());
  }

  private async loadGaussianAvatar() {
    const plyPath = this.container.dataset.plyPath || '/assets/avatar/GS_canonical.ply';
    const gvrmPath = this.container.dataset.gvrmPath;

    console.log('[AvatarRenderer] Loading 3DGS avatar:', { plyPath, gvrmPath });

    try {
      // GaussianSplats3D DropInViewer for Three.js integration
      this.viewer = new GaussianSplats3D.DropInViewer({
        gpuAcceleratedSort: true,
        sharedMemoryForWorkers: false,
        dynamicScene: true,
        sceneRevealMode: GaussianSplats3D.SceneRevealMode.Instant,
        splatRenderMode: GaussianSplats3D.SplatRenderMode.ThreeD
      });

      // Add viewer to Three.js scene
      this.scene.add(this.viewer);

      // Load the PLY splat scene
      await this.viewer.addSplatScene(plyPath, {
        splatAlphaRemovalThreshold: 5,
        position: [0, -0.3, 0],
        rotation: [0, 0, 0, 1],
        scale: [1.5, 1.5, 1.5]
      });

      this.splatScene = this.viewer.getSplatScene(0);

      this.fallback.classList.add('hidden');
      this.isLoaded = true;
      this.updateStatus('loaded', '3DGS Ready');

      // Start animation loop
      this.animate();

      setTimeout(() => {
        this.status.classList.remove('visible');
      }, 2000);

      console.log('[AvatarRenderer] 3DGS avatar loaded successfully with GaussianSplats3D');

    } catch (error) {
      console.warn('[AvatarRenderer] GaussianSplats3D failed, trying PLY fallback:', error);
      await this.loadPLYFallback(plyPath);
    }
  }

  // Fallback to Three.js Points if GaussianSplats3D fails
  private async loadPLYFallback(plyPath: string) {
    try {
      const response = await fetch(plyPath);
      const arrayBuffer = await response.arrayBuffer();

      const { PLYLoader } = await import('three/examples/jsm/loaders/PLYLoader.js');
      const loader = new PLYLoader();
      const geometry = loader.parse(arrayBuffer);

      const material = new THREE.PointsMaterial({
        size: 0.008,
        vertexColors: true,
        sizeAttenuation: true,
        transparent: true,
        opacity: 0.95
      });

      const points = new THREE.Points(geometry, material);
      points.position.set(0, -0.3, 0);
      points.scale.setScalar(1.5);
      this.scene.add(points);

      this.fallback.classList.add('hidden');
      this.isLoaded = true;
      this.updateStatus('loaded', '3DGS Ready (Fallback)');
      this.animate();

      setTimeout(() => {
        this.status.classList.remove('visible');
      }, 2000);

      console.log('[AvatarRenderer] PLY fallback loaded');
    } catch (e) {
      console.error('[AvatarRenderer] All loading methods failed:', e);
      this.updateStatus('error', 'Using 2D Avatar');
      setTimeout(() => {
        this.status.classList.remove('visible');
      }, 3000);
    }
  }

  private updateStatus(state: 'loading' | 'loaded' | 'error', text: string) {
    this.status.classList.remove('loaded', 'error');
    this.status.classList.add('visible', state);
    const textEl = this.status.querySelector('.status-text');
    if (textEl) textEl.textContent = text;
  }

  private handleResize() {
    const rect = this.container.getBoundingClientRect();
    this.camera.aspect = rect.width / rect.height;
    this.camera.updateProjectionMatrix();
    this.renderer.setSize(rect.width, rect.height);
  }

  private animate() {
    this.animationFrameId = requestAnimationFrame(() => this.animate());

    if (this.isSpeaking) {
      this.updateLipSyncFromAudio();
      this.applyVisemeAnimation();
    }

    // Update GaussianSplats3D viewer
    if (this.viewer) {
      this.viewer.update();
    }

    this.controls.update();
    this.renderer.render(this.scene, this.camera);
  }

  // Audio analysis for lip sync - maps frequencies to visemes
  private updateLipSyncFromAudio() {
    if (!this.audioAnalyser) {
      // No audio - use sine wave animation
      const time = performance.now() / 1000;
      this.lipSyncParams.jawOpen = Math.abs(Math.sin(time * 6)) * 0.5;
      this.lipSyncParams.currentViseme = this.getVisemeFromTime(time);
      return;
    }

    const dataArray = new Uint8Array(this.audioAnalyser.frequencyBinCount);
    this.audioAnalyser.getByteFrequencyData(dataArray);

    // Frequency bands for viseme detection
    const lowFreq = dataArray.slice(0, 8).reduce((a, b) => a + b, 0) / 8;    // 0-300Hz
    const midLowFreq = dataArray.slice(8, 16).reduce((a, b) => a + b, 0) / 8; // 300-600Hz
    const midFreq = dataArray.slice(16, 32).reduce((a, b) => a + b, 0) / 16;  // 600-1200Hz
    const highFreq = dataArray.slice(32, 64).reduce((a, b) => a + b, 0) / 32; // 1200-2400Hz

    // Jaw opening from overall volume
    const volume = (lowFreq + midLowFreq + midFreq) / 3;
    this.lipSyncParams.jawOpen = Math.min(volume / 150, 1);

    // Viseme detection based on frequency characteristics
    // a: low frequency dominant (open mouth)
    // i: high frequency dominant (wide mouth)
    // u: mid-low frequency (rounded lips)
    // e: mid frequency (slightly open)
    // o: low-mid frequency (rounded)

    if (volume < 10) {
      this.lipSyncParams.currentViseme = 'neutral';
    } else if (lowFreq > midFreq && lowFreq > highFreq) {
      this.lipSyncParams.currentViseme = lowFreq > midLowFreq ? 'a' : 'o';
    } else if (highFreq > midFreq) {
      this.lipSyncParams.currentViseme = 'i';
    } else if (midLowFreq > midFreq) {
      this.lipSyncParams.currentViseme = 'u';
    } else {
      this.lipSyncParams.currentViseme = 'e';
    }

    // Update viseme weights with smoothing
    const targetWeights = { a: 0, i: 0, u: 0, e: 0, o: 0 };
    if (this.lipSyncParams.currentViseme !== 'neutral') {
      targetWeights[this.lipSyncParams.currentViseme] = this.lipSyncParams.jawOpen;
    }

    // Smooth interpolation
    const smoothing = 0.3;
    for (const v of ['a', 'i', 'u', 'e', 'o'] as const) {
      this.visemeWeights[v] += (targetWeights[v] - this.visemeWeights[v]) * smoothing;
    }
    this.visemeWeights.jawOpen = this.lipSyncParams.jawOpen;
  }

  // Cycle through visemes when no audio
  private getVisemeFromTime(time: number): 'a' | 'i' | 'u' | 'e' | 'o' | 'neutral' {
    const visemes: ('a' | 'i' | 'u' | 'e' | 'o')[] = ['a', 'i', 'u', 'e', 'o'];
    const index = Math.floor((time * 3) % visemes.length);
    return visemes[index];
  }

  // Apply viseme animation to the 3DGS model
  // Note: Full blendshape support requires GVRM format
  // Current: Transform-based approximation
  private applyVisemeAnimation() {
    if (!this.viewer || !this.splatScene) return;

    const jaw = this.visemeWeights.jawOpen;
    const time = performance.now() / 1000;

    // Transform-based lip sync animation
    // Y-scale for jaw opening, subtle rotation for expression
    const scaleY = 1.5 * (1 + jaw * 0.08);
    const rotY = Math.sin(time * 0.4) * 0.03;
    const rotX = Math.sin(time * 0.25) * 0.02;

    // Apply transform to the splat scene
    // GaussianSplats3D uses position/rotation/scale arrays
    try {
      // Access the splat mesh transform
      if (this.viewer.children && this.viewer.children.length > 0) {
        const splatMesh = this.viewer.children[0];
        if (splatMesh) {
          splatMesh.scale.set(1.5, scaleY, 1.5);
          splatMesh.rotation.y = rotY;
          splatMesh.rotation.x = rotX;
        }
      }
    } catch (e) {
      // Fallback: transform the viewer itself
      this.viewer.scale.set(1, 1 + jaw * 0.05, 1);
      this.viewer.rotation.y = rotY;
    }
  }

  public startSpeaking(audioElement?: HTMLAudioElement) {
    console.log('[AvatarRenderer] startSpeaking called, isLoaded:', this.isLoaded);
    this.isSpeaking = true;
    this.container.classList.add('speaking');

    if (audioElement && !this.audioAnalyser) {
      try {
        this.audioContext = new AudioContext();
        const source = this.audioContext.createMediaElementSource(audioElement);
        this.audioAnalyser = this.audioContext.createAnalyser();
        this.audioAnalyser.fftSize = 256;
        source.connect(this.audioAnalyser);
        this.audioAnalyser.connect(this.audioContext.destination);
        console.log('[AvatarRenderer] Audio analyser connected for lip sync');
      } catch (e) {
        console.warn('[AvatarRenderer] Could not create audio analyser:', e);
      }
    }
  }

  public stopSpeaking() {
    console.log('[AvatarRenderer] stopSpeaking called');
    this.isSpeaking = false;
    this.container.classList.remove('speaking');

    // Reset viseme weights
    this.visemeWeights = { a: 0, i: 0, u: 0, e: 0, o: 0, jawOpen: 0, blink: 0 };
    this.lipSyncParams = { jawOpen: 0, lipSync: 0, currentViseme: 'neutral' };

    // Reset transform
    if (this.viewer) {
      this.viewer.scale.set(1, 1, 1);
      this.viewer.rotation.set(0, 0, 0);
    }
  }

  // Manual viseme control (for external lip sync systems)
  public setViseme(viseme: 'a' | 'i' | 'u' | 'e' | 'o' | 'neutral', weight: number = 1) {
    // Reset all
    this.visemeWeights.a = 0;
    this.visemeWeights.i = 0;
    this.visemeWeights.u = 0;
    this.visemeWeights.e = 0;
    this.visemeWeights.o = 0;

    if (viseme !== 'neutral') {
      this.visemeWeights[viseme] = weight;
      this.visemeWeights.jawOpen = weight * 0.5;
    }
  }

  public updateLipSync(params: { jawOpen?: number; viseme?: 'a' | 'i' | 'u' | 'e' | 'o' | 'neutral' }) {
    if (params.jawOpen !== undefined) this.visemeWeights.jawOpen = params.jawOpen;
    if (params.viseme) this.setViseme(params.viseme, params.jawOpen || 0.5);
  }

  public get is3DGSLoaded(): boolean {
    return this.isLoaded;
  }

  public get currentViseme(): string {
    return this.lipSyncParams.currentViseme;
  }

  public dispose() {
    cancelAnimationFrame(this.animationFrameId);

    if (this.audioContext) {
      this.audioContext.close();
    }

    if (this.viewer) {
      this.viewer.dispose();
      this.scene.remove(this.viewer);
    }

    this.renderer.dispose();
    this.controls.dispose();
  }
}

// Initialize on DOM ready
document.addEventListener('DOMContentLoaded', () => {
  const container = document.querySelector('.avatar-renderer') as HTMLElement;
  if (container) {
    const renderer = new AvatarRenderer(container);
    (window as any).avatarRenderer = renderer;
    console.log('[AvatarRenderer] Initialized and exposed as window.avatarRenderer');
  }
});

export { AvatarRenderer };
</script>
